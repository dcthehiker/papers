
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>GraphicsAssist- Visual Artist’s Assistant for Narration-to-Illustrative Graphic Composition</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="1" id="mark-794d1b97-fefc-47d7-a2f2-88024b4b81ad" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div>E-ISSN 2456-978X | Peer-Reviewed &amp; Internationally Indexed Journal CHITROLEKHA</div></div></div><div><div><div></div></div></div><div><div><div>JOURNAL ON ART AND DESIGN</div></div></div><div><div><div></div></div></div><div><div><div>Indexed and/or abstracted in</div></div></div><div><div><div></div></div></div><div><div><div>ProQuest Arts Premium Database | EBSCO Art &amp; Architecture Source Database | Index Islamicus (Brill)</div></div></div><div><div><div></div></div></div><div><div><div>Dimensions (Digital Science) | EZB (Barcelona) | Fatcat | Scilit | COBISS</div></div></div><div><div><div></div></div></div><div><div><div>d https://doi.org/10.21659/cjad.81.v8n109 | Vol 8 No 12024 | Dynamic Impact (Dimensions</div></div></div><div><div><div></div></div></div><div><div><div>Research article</div></div></div><div><div><div></div></div></div><h1><div><div>GraphicsAssist: Visual Artist's Assistant for Narration-to-Illustrative  Graphic Composition</div></div></h1><div><div><div></div></div></div><div><div><div>Suma Dawn</div></div></div><div><div><div></div></div></div><div><div><div>Dept. of CSE &amp; IT, Jaypee Institute of Information Technology, A-10, Sec-62, Noida-201309, India</div></div></div><div><div><div></div></div></div><h2><div><div>Abstract:</div></div></h2><div><div><div></div></div></div><div><div><div>Storytelling is an art that very few can carry with ease, putting in proper context the feelings and emotions of the characters, their setting, interactions, etc. This work attempts to recreate storytelling to form a graphic illustration for users so that the essence of the story is not lost in transference from one person to another, from storyteller to graphic illustration reader/user. ’GraphicsAssist’ is an artist assistance system for graphic artists to help them generate certain options for usage as graphic frames. Text analysis has been exploited to understand contextual requirements or storytelling. Natural language-based learning has been applied to this requirement. The transfer-learning-based model is used to find suitable matches for the characters and visual environment settings. Illustrative drawings and their tags are considered a part of the frame database. Once the narration entities of the story are extracted, they are matched with the drawing tags and presented to the designer for framing and then panelling of graphic illustration. The chosen frames are displayed in panels in grid format. In its current form, manipulation of the selected drawings for the task of forming frames is, as yet, restricted. The aesthetic transition from frame to frame is yet lacking and may be integrated into later works. The work of this artist's assistance system is validated by the test group, which gave feedback on some pre-defined parameters. Though the system has its drawbacks, it is the first step towards such a system that provides a computational translation model for translating a textual narration to a set of visual composition frames. This work aims to capture the nuances, characters, emotions, story setting, etc., from textual narration as an input of the story and convert it to the graphic illustration in 2D form by incorporating illustrative drawings as its visual translation.</div></div></div><div><div><div></div></div></div><div><div><div>Keywords: Textual Narration; NLP, NER; Image Tagging; Narratology; Feature extraction; Graphic frames; Transfer Learning; Visual Narration</div></div></div><div><div><div></div></div></div><div><div><div>SUSTAINABLE GOALS Better Education</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="2" id="mark-74f790a6-8cce-48fc-bfbe-b1cbe2f7f435" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><h2><div><div>I. Introduction</div></div></h2><div><div><div></div></div></div><div><div><div>Traditional literature is an incessant storehouse of stories. Indian folklore, with its diverse cultural bends, teachings, and environmental settings, consists of an amaranthine range of short stories and tales. Learning and entertainment are intertwined [1], [2]. While reading a story, the reader visualizes the story narration. This visualization imprinted on her mind allows for multiple interpretations [3 - 6]. These visuals include characters, their structures, the environment in which the story is set, the mood of the various characters, emotional changes, and others, each of which leaves a deep impact on the mind and affects the enjoyment of the story [7-9].</div></div></div><div><div><div></div></div></div><div><div><div>Traditional Indian graphics such as Pran's "Chacha Choudhury" and recent additions such as "Chota Bheem" have undergone a sea of change, starting from changing hand drawings to using illustrative designing tools, changes in panel depictions and others. "Graphic-Con" and studios such as Marvel's Graphic World, Disney, Pixar, Dreamworks and many more have brought graphics into high-grossing movies. Such narration visualizations are compelling the mapping from the narrative zone to presenting a computationally viable graphic engine. For example, in Voki.com, shy students use avatars to present their ideas and voice their thoughts. However, these avatars do not display many facial expressions, which may render them unfriendly to the users. Also, there are no automated or semi-automated systems or tools that can allow for the creation of a graphic frame that may allow for background settings and foreground characters along with the placement of relevant dialogue boxes. Further, there is no open-source software that considers a story narration as an input to form a caricature or a character, and nor are there any tools to extract an appropriate background by sensing the context of the story. This work proposes to semi-automate story narration and formalizes it as a graphic illustration to enhance the story experience. Such a graphic engine would encourage artists, writers, and designers to form and map their stories into a well-panelled graphic illustration formation for initial representations.</div></div></div><div><div><div></div></div></div><div><div><div>In this work, we propose to use textual narration as input to generate a graphic illustration. Firstly, Natural Language Processing (NLP) is used to extract the elements of the story and understand its various phases. Secondly, interpret the elements to feature descriptors. Thirdly, appropriate visual elements should be retrieved to map feature descriptors to match the tags of the illustrator's drawings. Finally, the selected illustrations are placed to form a visual graphic representation. The matching process uses transfer learning based on the feature space remapping function. After the initial query retrieval of all the images with the relevant tags, the similarity measure further fine-tunes the search result. The user can then choose any of the illustrations retrieved from the tagged image database.</div></div></div><div><div><div></div></div></div><div><div><div>The next section depicts the current related works with respect to theories of textual story narration, Part-of-speech tagging framework, and image tagging. Section III presents the proposed methodology for translating a story from textual narration to graphic illustration. The experimental results are detailed in Section IV. The feedback received from the test target group is aggregated in section V. Certain further improvements are mentioned in section VI. Section VII concludes the paper.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="3" id="mark-4b13cfbb-c283-4890-83d1-978189cfe9f6" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><h2><div><div>II. LITERATURE SURVEY</div></div></h2><div><div><div></div></div></div><div><div><div>A short story may be considered to be a short piece of fiction. These are among the oldest forms of storytelling. Ancient texts and mythologies were considered to be a storehouse of stories and were passed on through oral language, so stories needed to be brief. Story narration is a fundamental form of communication. This is not only for entertainment but also for knowledge transmission through stories.</div></div></div><div><div><div></div></div></div><div><div><div>The computational transition from narration to visual depiction, especially as a graphic illustration, involves two basic dimensions of data. These dimensions must be processed and mapped the latter to get resultant data. One aspect of the system involves textual narration and story processing, while the other aspect involves mapping the illustrations to present an appropriate frame. Presentation of a story as a set of image frames in terms of graphic illustrations involves understanding the narration aspects as well as relevant computational requirements. The resultant frames should have simplicity and rhythm to generate a graphic illustration. While creating the characters and their environments, story details are important. For adequate visual effects, a proper panel of frames is of momentous importance for composition and page layout [10 - 18]. Apart from following the 12 principles of Animation [19], frame formation is also very important.</div></div></div><div><div><div></div></div></div><div><div><div>Recent studies in narration showcase the usage of various parameters for translation and transmission. These consist of setting, characters, and plots and point-of-views. 'Settings' gives an overview of the basics of the story's actions of time and place of happening, such as geographic locations, customs, beliefs, ideas and values, rituals, etc. These help give the story wholesomeness. These influence the narration, era, costumes, characters' reactions to various behaviours and protocols, mood and atmosphere. It sets the basic feeling of the story, climate conditions such as weather and temperature, populations and ancestral influences. Computationally, depicting setting parameters is very difficult, and its visualisation can only be through background and character behaviour. Characters play the plot so that the sequence of events can be advanced. Depending on the writer's choice, characters can be animals, people, or animate things. The story text provides information about various characters and their attributes, such as name, gender, and others. As the story progresses, various information such as what the secondary character wants from the protagonist, how the secondary character feels about the protagonist, where the story takes place - setting/description of the place - from adjectives, emotional adjectives, adjectives to describe objects; character descriptions; occasions; verbs and their associations with characters; nouns to compare characters; organization; time of the day; story theme, etc. A story usually processes through five stages of the plot, namely, exposition, rising action, climax, falling action and resolution. The aspects can be propelled via the conflicts, which may be emotional, intellectual, moral, etc. Plots may be presented chronologically, in flashbacks, or as media-res. Plot extraction may be presented computationally as a set of relational tuples based on neural conferencing and NLP.</div></div></div><div><div><div></div></div></div><div><div><div>Computational processing of textual narration and story processing involves the domain of NLP, which is augmented by Deep Learning approaches. These allow for the extraction of characters, backdrop, theme, organisation name, settings, etc. Named-Entity Recognition (NER) using Deep Learning having skip-gram and bag of words methods has given good results. On the other hand, tagging of illustrations involves associating multiple tags to these images, which can later be applied to mapping from narration storyline to illustration frames. Initial image classification is performed using techniques used by authors in [20 - 25]. Further, automatic image annotation, to get better tagging data, is also performed using methods as shown by [26]. Relevant images are then processed for their associated sentiments using the method mentioned by authors [27], [28].</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="4" id="mark-ca5f00e2-25f6-483b-8edd-664ea6758351" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><div><div><div>The limitations found in the reading include the fact that there is, currently, no automatic or semiautomatic system for assisting illustration artists in translating a textual narration to a graphic illustration setting. The methodology presented in this work is a step towards reducing this gap.</div></div></div><div><div><div></div></div></div><h2><div><div>III. Methodology</div></div></h2><div><div><div></div></div></div><div><div><div>i Extraction of nouns, named entities</div></div></div><div><div><div></div></div></div><div><div><div>ii. noun - feature descriptors</div></div></div><div><div><div></div></div></div><div><div><div>iii. extract appropriate characters/ objects for visual depiction.</div></div></div><div><div><div></div></div></div><div><div><div>iv. list of various emotions</div></div></div><div><div><div></div></div></div><div><div><div>v. list of body postures</div></div></div><div><div><div></div></div></div><div><div><div>vi. sound words</div></div></div><div><div><div></div></div></div><div><div><div>vii. dialogue and conversation elements extraction</div></div></div><div><div><div></div></div></div><div><div><div>viii mapping various elements to characters.</div></div></div><div><div><div></div></div></div><div><div><div>ix framing</div></div></div><div><div><div></div></div></div><div><div><div>x. storyboarding</div></div></div><div><div><div></div></div></div><div><div><div>The intent, as mentioned in the sections earlier, is to form a graphic illustration from a textural narration. There is no automatic process for this, however, a semi-automatic procedure has been proposed in this work.</div></div></div><div><div><div></div></div></div><div><div><div>If the input is a textual narration consisting of short sentences, the basic steps may be enumerated as follows:</div></div></div><div><div><div></div></div></div><div><div><div>(i) Extraction of story elements -</div></div></div><div><div><div></div></div></div><div><div><div>(a) Extraction of Keywords Named-Entity-Recognition (NER) is also important.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="5" id="mark-68425986-17ce-4e86-9073-d4be46ee501c" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01916976-5bd9-7436-b900-c75fd74caf76_5.jpg?x=369&amp;y=183&amp;w=911&amp;h=844"></div></div><div><div><div></div></div></div><div><div><div>In this stage, all keywords can be extracted. These keywords will help in extracting the story elements. Input to the system is textual narration or a short story. This is fed to the POS-NER module [29] for the extraction of characters, places, time, and other entities. This stage is followed by establishing relationship rules. These entities are mapped to the story elements, i.e., Characters - their names, their associated adjectives, gender, person/animal; Association between primary and secondary characters; Setting, i.e., physical look-and-feel of the place where events are taking place; Verbs and elements that are associated with activities; Events and their timings. The nouns extracted through the NLP parser help select the images to be used by matching with their associated tags [30 - 34], [38]. In any given sentence, each word belongs to a definite grammatical class such as noun, verb, adjective, adverb, article, etc [29], [35], [36], [37]. A POS tag set is a set of terms representing POS classes, including a simple tag set and complex morphological information. Most words have a preferred class. For English, Brown tag set and Penn tag set are widely known POS tagging sets. Initially, POS taggers assign one POS tag to each input token. There exist multiple ways of breaking down POS tagging [29], [26]. Apart from POS tagging,</div></div></div><div><div><div></div></div></div><div><div><div>(b) Extraction of Narrative events.</div></div></div><div><div><div></div></div></div><div><div><div>In a textual narration, an event may be represented as a tuple constituting the expressions or in terms of what,who,why,where,when,and how <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-n"><mjx-c class="mjx-c57"></mjx-c></mjx-mi></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mn>5</mn><mrow data-mjx-texclass="ORD"><mstyle scriptlevel="0"><mspace width="0.278em"></mspace></mstyle><mi mathvariant="normal">W</mi></mrow><mn>1</mn><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">H</mi></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> extracted from the parts of speech tags. The Parts-of-Speech (POS) tags and Named-Entity Relationships (NERs) can depict the components and their relationships, such as subjects, verbs and objects. An event may be expressed as a tuple <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> Character: Verb,Dependency <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo></math></mjx-assistive-mml></mjx-container> . Dependency depicts the role of a character in relation to the verb or preposition. If the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-n"><mjx-c class="mjx-c57"></mjx-c></mjx-mi></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mrow data-mjx-texclass="ORD"><mstyle scriptlevel="0"><mspace width="0.278em"></mspace></mstyle><mi mathvariant="normal">W</mi></mrow><mn>1</mn><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">H</mi></mrow></math></mjx-assistive-mml></mjx-container> can be extracted,it would present the elements such as subject, predicate, object, location, time, etc. These are then used to find the grammatical relationship between words and sub-divide phrases to extract the action verb and its relationship to other components. Standardization method, such as lemmatization, is used to reduce the complexity of sentences and make analysis much more efficient by restoring morpheme units to their basic forms [44]. NER may also be used to depict the relationship.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="6" id="mark-aff0ee93-a927-4232-9514-22f59812d747" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><h2><div><div>Example:</div></div></h2><div><div><div></div></div></div><div><div><div>Narrative text: Shivika was walking along a road</div></div></div><div><div><div></div></div></div><div><div><div>Extracted Events: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="4" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c65"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c73"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">e</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">s</mi></mrow></mrow></msub></math></mjx-assistive-mml></mjx-container> (Shivika), <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="5" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c65"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c76"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">e</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">v</mi></mrow></mrow></msub></math></mjx-assistive-mml></mjx-container> (walk), <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="6" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c65"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6F"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">e</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">o</mi></mrow></mrow></msub></math></mjx-assistive-mml></mjx-container> (road)</div></div></div><div><div><div></div></div></div><div><div><div>(c) Extraction of Narrative chain.</div></div></div><div><div><div></div></div></div><div><div><div>A Narrative chain is said to be a partially ordered set of narrative events that share a common character. These chains can contain a set of events and a binary relationship between the event set. These POS tags, Narration events and chains are then converted to feature descriptors.</div></div></div><div><div><div></div></div></div><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01916976-5bd9-7436-b900-c75fd74caf76_6.jpg?x=197&amp;y=850&amp;w=1163&amp;h=377"></div></div><div><div><div></div></div></div><div><div><div>Figure 1: An illustration of high level plot point extraction.</div></div></div><div><div><div></div></div></div><div><div><div>(ii) Next, dialogues from the narrative structure are to be extracted. Text in the narration may contain dialogues, either stated directly or indirectly. Dialogues are the words spoken by characters in the narration, whereas action sequences help give direction to how the characters perform or react. A dialogue may be said to consist of two parts - characters who are uttering the dialogue and the dialogue text itself. Dialogues are extracted using the context of the sentences themselves. Verbs like 'said' and 'told' and their various forms, when used with proper nouns, may give rise to dialogue, especially if the said text is enclosed within double quotations.</div></div></div><div><div><div></div></div></div><div><div><div>(iii) The third stage consists of a mapping feature descriptor to image tags.</div></div></div><div><div><div></div></div></div><div><div><div>(iv) The fourth stage is that of choosing passable illustrations pertaining to image tags.</div></div></div><div><div><div></div></div></div><div><div><div>(v) The fifth stage sets the arrangement of the image as required by the story narrative and associating the dialogues with the characters.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="7" id="mark-1353aadd-2cbd-4ba1-937c-0cac574356d1" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><div><div><div>The flow between these steps can be depicted as shown in Figure 1.</div></div></div><div><div><div></div></div></div><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01916976-5bd9-7436-b900-c75fd74caf76_7.jpg?x=215&amp;y=238&amp;w=1158&amp;h=840"></div></div><div><div><div></div></div></div><div><div><div>Figure 3: "GraphicsAssist" Module Diagram</div></div></div><div><div><div></div></div></div><h2><div><div>A. Narration to POS-NER tags</div></div></h2><div><div><div></div></div></div><h2><div><div>B. Associating Emotions to Characters</div></div></h2><div><div><div></div></div></div><div><div><div>The adjectives play an important role in establishing a relationship between objects and/or nouns by demonstrating ownership. In this case, possessive adjectives are most suitable. Descriptive adjectives associated with their noun counterparts present their different quantifiers. These present the associated noun's opinion, action, state or quality. These quantifiers may be colour, size, taste, and texture, among others. These adjectives are used for the portrayal of emotion related to the main narration of the characters. The six emotions of happy, sad, angry, surprise and indefinite adjectives give some idea of the count - some, few, many, etc. In our work, these have been used alongside nouns for the depiction of various narration characters. These are usually used in the representation of secondary characters.</div></div></div><div><div><div></div></div></div><div><div><div>The WordNet list of adjectives is used (WordNet Database) as it also gives their associated sentiments. These basic sentiments and emotions are part of the matching keywords that are used to retrieve characters.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="8" id="mark-7a8ce81e-c6a4-4f83-935a-a42dc2415d4a" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><h2><div><div>C. Selection of Characters from Illustrative Drawing/Illustrations.</div></div></h2><div><div><div></div></div></div><div><div><div>The illustration dataset is created by crawling various sites. Each of these collected illustrations is then mapped to image tags. Image classification and tagging are implemented using the transfer learning approach.</div></div></div><div><div><div></div></div></div><div><div><div>Convolution Neural Network (CNN) has emerged as a leading architecture for image recognition and classification. Other methods, too, have shown promising results for tagging illustrative images [20], [21], [22]. CNN model can be controlled by varying its depth and breadth. Deep CNN has further fueled its usage. The input image is fed to the network that undergoes several stages of convolution and pooling. Then, the representations from these operations feed the connected layer, which finally outputs the class label [24], [25]. Feature extraction is performed by the convolution layers. This can be represented as: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="7" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.182em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mstyle scriptlevel="0"><mspace width="0.278em"></mspace></mstyle><msub><mrow data-mjx-texclass="ORD"><mi>Y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow></msub><mo>=</mo><mi>f</mi><mo>.</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>W</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow></msub><mo>∗</mo><mi>x</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="8" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c59"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Y</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></mrow></msub></math></mjx-assistive-mml></mjx-container> is the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="9" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow><mrow data-mjx-texclass="ORD"><mtext>th&nbsp;</mtext></mrow></msup></math></mjx-assistive-mml></mjx-container> output feature map, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="10" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></mjx-assistive-mml></mjx-container> is the input image, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="11" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>W</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> is the convolution filter related to the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="12" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow><mrow data-mjx-texclass="ORD"><mtext>th&nbsp;</mtext></mrow></msup></math></mjx-assistive-mml></mjx-container> feature map, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mo>∗</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> is the 2D convolution operator,and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="14" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>r</mi><mi>e</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>n</mi><mi>o</mi><mi>n</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><mo>.</mo><mrow data-mjx-texclass="ORD"><mi>T</mi><mi>h</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>l</mi><mi>p</mi></mrow></math></mjx-assistive-mml></mjx-container> reduce the spatial resolution of the feature maps and attain spatial invariance to input distortions and translations. The aggregation layers are used to transmit the maximum value within each receptive field to the next layer. This is represented as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="15" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.182em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munder space="4"><mjx-row><mjx-base style="padding-left: 0.53em;"><mjx-texatom texclass="OP"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c211C TEX-FR"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>Y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP">max</mo></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mo>,</mo><mi>q</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∈</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">ℜ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>j</mi></mrow></msub></mrow></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi><mi>p</mi><mi>q</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="16" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.182em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>Y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi><mi>i</mi><mi>j</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> denotes the output of the pooling operation associated with the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow><mrow data-mjx-texclass="ORD"><mtext>th&nbsp;</mtext></mrow></msup></math></mjx-assistive-mml></mjx-container> feature, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="18" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi><mi>p</mi><mi>q</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> represents the element at the location <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="19" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mo>,</mo><mi>q</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> contained by the pooling region <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="20" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c211C TEX-FR"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">ℜ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>j</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> . This region is the receptive field around the position <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="21" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . Multiple convolutional and pooling layers are then stacked on top of each other to extract more abstract feature representations. They are followed by fully connected layers to perform high-level classification. This is performed by implementing the softmax operator or support vector machine. After this, CNN training takes place to adjust its biases and weight parameters. The training is implemented using back-propagation that determines the gradient of the performance function. The initial matching involves using dictionary-based searching for particular tags. However, there are many cases where proper tagged data is not available, or the coarse selection presents a huge set of data for depiction. The selection in such cases is done using transfer learning with a feature-space remapping function mechanism.</div></div></div><div><div><div></div></div></div><div><div><div>Transfer learning works on the basic principle of learning from a pre-trained model and transferring or applying this knowledge to the non-trained smaller dataset. For objects classified with the earlier used CNN, the initial convolution layers of the network are frozen, and the latter remaining layers are used for making a prediction. The convolutional layers are used to extract low-level features such as strong edges, regular patterns, and gradient functions. The final layers are used for extracting specific features or patterns that help in finer classification. This allows for classifying illustrations that may have an affine transformation mapping or a limited degree of non-affine transformations, such as skew or wrap functions associated with them [40], [41]. An ensemble classifier is then built by defining the mapping function between the target domain and source domain features. This classifier, called "Ensemble Learning via Feature-Space Remapping" (ELFSR), is used to predict the class of illustrative drawings.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="9" id="mark-f56944f7-353b-4a5b-be94-a2960fddaec0" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><h2><div><div>D. Visualization Canvas Panel Frames</div></div></h2><div><div><div></div></div></div><div><div><div>The formalism of textual narration to graphic illustration depiction is not yet streamlined; rather, it follows an intuitive and simplistic learning model. Initially, a database of illustrations is created so as to cater to the requirement of selecting appropriate characters for depiction. Presently, the set of backgrounds is very constrained. First, all the elements of the narration (Story Elements, as mentioned in Figure 1) have been identified. The image database is a pre-classified, priory-tagged set of illustrations. Since the aim is to formalize the narration as a graphic illustration, illustrative drawings have been considered in the database. The illustrations are of the size of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="22" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-var"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi><mjx-mi class="mjx-var"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-var"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi><mjx-mi class="mjx-var"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c50"></mjx-c><mjx-c class="mjx-c50"></mjx-c><mjx-c class="mjx-c49"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow><mrow data-mjx-texclass="ORD"><mi data-mjx-alternate="1">′</mi><mi data-mjx-alternate="1">′</mi></mrow></msup><mo>×</mo><msup><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow><mrow data-mjx-texclass="ORD"><mi data-mjx-alternate="1">′</mi><mi data-mjx-alternate="1">′</mi></mrow></msup><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>72</mn></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">PPI</mi></mrow></mrow></math></mjx-assistive-mml></mjx-container> , JPEG images with sizes ranging from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="23" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c><mjx-c class="mjx-c42"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">KB</mi></mrow></mrow></math></mjx-assistive-mml></mjx-container> to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="24" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4D"></mjx-c><mjx-c class="mjx-c42"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>56</mn></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">MB</mi></mrow></mrow></math></mjx-assistive-mml></mjx-container> . Another design constraint is to use the mediumblob datatype for easy storage.</div></div></div><div><div><div></div></div></div><div><div><div>Story elements are then converted to entities, i.e., features and their feature vectors. Each feature has a feature type and its relevant feature vector set. Feature types are assumed to be of 4 categories - (i) characters, (ii) settings, (iii) adjectives to describe a particular character and (iv) association map. Character feature type has the following associated parameters - type (person/animal/robot/others), gender (male/female), and order (protagonist / secondary). The association map is an adjacency matrix that describes the relationship between various characters, such as feelings and emotions, and their strength. It uses the concept of a bag of words, sentiment labelling, and its associated strength with the characters. This is an n-dimension matrix. The story element description is then mapped with the tags of images.</div></div></div><div><div><div></div></div></div><div><div><div>For our current scenario, we have used only characters as foreground expressions and their tone/emotion. Constraints dictated that background depiction be delayed for this phase of the work. Further, dialogues have also been withheld. Though the placement of the speech balloon is currently non-consequential, it would be important while developing a book. Onomatopoeic words, i.e., words associated with noises such as human sounds, animal sounds, vehicle sounds, and others, are also depicted in speech balloons, and placement is near to characters so as to redesign the facial expression of the characters. Motions and their adverbs are extracted and depicted using minor motion lines and curves [16], [17]. Most extractions are done using the Named-Entity-Recognition module and stored in vectors.</div></div></div><div><div><div></div></div></div><h2><div><div>IV. EXPERIMENTATION AND RESULTS</div></div></h2><div><div><div></div></div></div><div><div><div>Since the latency of the system should be kept as low as possible to have a decent flow in the working of the application, for our current experimentations, a few constraints have been maintained: (a) the textual narrations have not been classified to any particular themes; (b) the narrations closely follow the POS [29], [35 - 37], (c) only textual narration has been used for the depiction of frames. No dialogues, sound words or exaggerations are incorporated; (d) it has been assumed that the narrations follow Freytag's pyramid for dramatic structure depiction; (e) frame modification is restricted, and the chosen image is assumed to be for foreground object only.</div></div></div><div><div><div></div></div></div><div><div><div>The image database contains images and tags. Image data extraction is based on mining their tags. Initially, a non-tagged illustration image database is formed. Currently, the database consists of illustrative drawings only, as the initial aim is to form frames of cartoon-type illustrations. As of now, the database has almost 3000 images tagged. These tagged images are mined to get approximate images for the placement within the frames. Currently, only the emotions of happy, sad, and neutral are considered. For animals, no such markers are yet used. At the current state, each tag has only one to two synonyms associated with it. The final application structure is shown in Figure 1.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="10" id="mark-72d112bc-41de-4df9-813a-a056101888bb" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><div><div><div>In this paper, we have depicted details of 2 such case studies. The details of these examples are discussed below. For each narration, POS tags are extracted. Further, enhanced universal dependencies are found, too. These have been extracted using the NLTK libraries. Entities and the association between them are also found. Stanford's NLP libraries [42], [43] have been implemented for the same. The narration text is also passed through NER to distinguish noun type entities of only person and location and numerical data of number and duration only. The tone of the narration at the document level, as well as sentence level, has been detected. Based on these extractions, the tags from the illustrative drawings are matched. The first screen presents only the top 10 matches to the user.</div></div></div><div><div><div></div></div></div><div><div><div>Table 1: Textual Narration; POS Tags; Entities and their associations.</div></div></div><div><div><div></div></div></div><div><div class="table-container"><table class="fixed-table"><thead><tr><th>Narration- Storyline</th><th>POS Tagging</th><th>Entities and their associations</th></tr></thead><tbody><tr><td>Shivika is a 10-year- old girl. She was walking along a road. Bhola, her dog, was walking with her. They were going to the market.</td><td>Shivika/NNP is/VBZ a/DT 10/CD year/NN old/JJ girl/NN ./. She/PRP was/VBD walking/VBG along/IN a/DT road/NN ./. Bhola/NNP „/, her/PRP<span>$</span> dog/NN ,/, was/VBD walking/VBG with/IN her/PRP ./. They/PRP were/VBD going/VBG to/TO the/DT market/NN./.</td><td>Character 1: Shivika (person) Associated Nouns – girl, age-10 yrs, road Verb: walking Character 2: Bhola (animal) Associated Noun – dog. Associated pronoun - her (previous root). Verb: Walking Pronoun: They, market Verb: going Location: Market. Emotion: Happy Final Verb: Walking</td></tr><tr><td>Radhika is a small girl of about 5 years. Bhavesh is her elder brother. She was eagerly waiting for her Bhavesh, near the door. Bhavesh was going to bring a toy-</td><td>Radhika/NNP is/VBZ a/DT small/JJ girl/NN of/IN about/RB 5/CD years/NNS ./. Bhavesh/NNP is/VBZ her/PRP<span>$</span> elder/JJ brother/NN ./.</td><td>Character 1: Radhika (person) Associated Nouns – girl, age-5 yrs. Character 2: Bhavesh (person), elder Associated Pronoun - her (previous root), bother. Pronoun: She, door</td></tr></tbody></table></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="11" id="mark-4414aef6-6fad-4c89-beba-4663d025cd67" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div class="table-container"><table class="fixed-table"><tbody><tr><td>car for her. Radhika was very excited.</td><td>She/PRP was/VBD eagerly/RB waiting/VBG for/IN Bhavesh/NNP ,/, near/IN the/DT door/NN ./. Bhavesh/NNP was/VBD bringing/VBG a/DT toy-car/NN for/IN her/PRP ./. Radhika/NNP was/VBD very/RB excited/JJ ./.</td><td>Verb: waiting Case: near- associated noun- door Character: Bhavesh (person), toy-car Associated Pronoun – her (person) Associated Verb - bringing Emotion: Happy</td></tr></tbody></table></div></div><div><div><div></div></div></div><div><div><div>Also, the tones of the narration at the document level and sentence level were found. This is required to gauge the general mood of the storyline. Table 2 depicts the tones at various narration levels for the considered test cases. The placing of the frames in the timeline follows the progress of the story narration itself without consideration of the time periods that may have been included within the narration itself.</div></div></div><div><div><div></div></div></div><div><div><div>Narration 1:</div></div></div><div><div><div></div></div></div><div><div><div>Frame 1: A girl in the foreground (background not included)</div></div></div><div><div><div></div></div></div><div><div><div>Frame 2: A dog,</div></div></div><div><div><div></div></div></div><div><div><div>Frame 3: Road</div></div></div><div><div><div></div></div></div><div><div><div>Frame 4: Marketplace.</div></div></div><div><div><div></div></div></div><div><div><div>Narration 2:</div></div></div><div><div><div></div></div></div><div><div><div>Frame 1: A girl in the foreground (background not included)</div></div></div><div><div><div></div></div></div><div><div><div>Frame 2: Brother</div></div></div><div><div><div></div></div></div><div><div><div>Frame 3: Toy Car Images tags and classification is presented in table 2. For our experimentation, we currently have 19271 images that were tagged and with their listed entities. These were used for the extraction of the illustrations by matching the keywords extracted from the story entities.</div></div></div><div><div><div></div></div></div><div><div><div>Table 2: Tones at various narration levels</div></div></div><div><div><div></div></div></div><div><div class="table-container"><table class="fixed-table"><thead><tr><th>Example</th><th>Document-level</th><th colspan="2">Sentence-level</th></tr></thead><tbody><tr><td rowspan="5">Narration 1</td><td rowspan="5">Joy: 0.56</td><td>0.86</td><td>They seemed very happy.</td></tr><tr><td>0</td><td>Shivika is a 10-year-old girl.</td></tr><tr><td>0</td><td>She was walking along a road.</td></tr><tr><td>0</td><td>Bhola, her dog, was walking with her.</td></tr><tr><td>0</td><td>They were going to the market.</td></tr><tr><td rowspan="5">Narration 2</td><td rowspan="5">Joy: 0.83 Confident: 0.68</td><td>0.99</td><td>Radhika was very excited.</td></tr><tr><td>0.57</td><td>Radhika is a small girl of about 5 years.</td></tr><tr><td>0.55</td><td>Bhavesh was going to bring a toy-car for her.</td></tr><tr><td>0</td><td>Bhavesh is her elder brother.</td></tr><tr><td>0</td><td>She was eagerly waiting for her Bhavesh, near the door.</td></tr></tbody></table></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="12" id="mark-65956d5d-981c-4a85-ad92-949b72cec45e" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div>Depiction of the illustrations retrieved by search keywords with respect to the tags associated with the images is depicted. The keywords extracted are used as search terms. After the retrieval of illustrations from the database as per their tags, the user can choose a particular image as per his requirements. Currently, manipulation of the images is not allowed. Once the user chooses the image, it is showcased as a single frame in the graphic illustration canvas. In the current experiment, the grid layout is used. Since only a horizontal illustration depiction is required, vertical divisions in the horizontal frameset are used. For the examples explained in this work, image frames used in the canvas layout are depicted in Table 3.</div></div></div><div><div><div></div></div></div><div><div><div>Table 3: Outcomes of “GraphicsAssist” system - Graphic illustration from Story Narration.</div></div></div><div><div><div></div></div></div><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01916976-5bd9-7436-b900-c75fd74caf76_12.jpg?x=192&amp;y=669&amp;w=1192&amp;h=1189"></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="13" id="mark-5b9f351b-adc9-498b-8985-b122366bdbe9" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><h2><div><div>V. VALIDATION OF THE PROPOSED ARTIST ASSISTANCE SYSTEM</div></div></h2><div><div><div></div></div></div><div><div><div>This graphic illustration designer tool was given to a target closed group for validating its usage. The feedback received was good, considering that this is an initial system as an artist and designer's assistant. The dataset was created earlier by prior crawling. The metric used included the following parameters as shown in table 4.</div></div></div><div><div><div></div></div></div><div><div><div>Table 4: Metric for subjective evaluation of the “Narration-to-Graphic” system.</div></div></div><div><div><div></div></div></div><div><div class="table-container"><table class="fixed-table"><thead><tr><th>Metric</th><th>Percentage based on user satisfaction</th></tr></thead><tbody><tr><td>Character detection</td><td>100%</td></tr><tr><td>Environment setting detection</td><td>62%</td></tr><tr><td>Extraction of relevant characters</td><td>100%</td></tr><tr><td>Frame panel formation</td><td>36%</td></tr></tbody></table></div></div><div><div><div></div></div></div><div><div><div>This proposed system is unique as it one of its kind. There is no other online or freeware system that can help as an artist’s assistant by assimilating various illustrations for a given storyline.</div></div></div><div><div><div></div></div></div><h2><div><div>VI. CONCLUSION</div></div></h2><div><div><div></div></div></div><div><div><div>The undertaking considered for presenting this literature starts with textual narration as an input to the system and getting a set of probable parameters for presenting an assortment of illustrative visuals. With the text, the systems dissect it into a set of entities - characters, emotions, and associations. These entities then form the basis of the search keyword. On the other end of the spectrum, an image database of illustrations is considered. After being tagged, these are to be used for the visual depiction. The process, though not perfect, is able to do certain cognitive justification to the narration storyline.</div></div></div><div><div><div></div></div></div><div><div><div>The various modules of this translation, from textual narration to graphic illustration visualization, are mentioned in detail. The initial phase consists of dissecting the narration to extract the story elements. These elements form the search vectors. These include the ramifications of each sentence, associating them with their corresponding parts of speech, and then applying NER formations and RACK structure approaches. The emotion is also extracted. These attribute sets are used as tags to match the illustration database. Further, a tagged illustration database is also maintained, wherein each illustration is tagged using multiple text tags of shape, size, colour, gender, and other classification attributes. The retrieval of images from the database is built on the query of the search keywords. The query then applied a finer matching technique based on transfer learning using the technique of ensemble learning via feature-space remapping. The user can choose any of the illustrations, as retrieved, to assemble the frames of the graphic illustration panel.</div></div></div><div><div><div></div></div></div><div><div><div>The "GraphicsAssist" application is a semi-automatic system for the generation of graphic frames from a given storyline. It is a very novel application that considers the multidisciplinary widely different domains of literature (narration) for computational prediction of frames for the depiction of a graphic illustration. Though this is a semi-automatic procedure and a finer approach may be required to generate a better graphic frame, the non-existence of such systems makes this application very novel in this domain. Numerous shortcomings are the basis of future enhancements that will optimize the system as well as make it aesthetically more acceptable to graphic artists.</div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="14" id="mark-98f6e5d7-51d2-49d5-989c-a1ad96ad84cf" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div></div></div></div><div><div><div></div></div></div><div><div><div>This work is useful for graphic artists as it can help in creating an initial phase of short graphic-illustrations. "GraphicsAssist" is a semi-automatic tool for finding entities that can depict the textual narration. Later, it may also be extended to accommodating graphic books and small animated series using appropriate animation principles and techniques, virtual/augmented environments, and games for use in entertainment, education, cultural and traditional literature preservation, amongst others. Numerous requirements pertaining to information extraction are being concluded satisfactorily, yet cognitive depiction and their evaluations are a huge hurdle. This may lead to reliability issues with respect to the cognition requirements of the narration.</div></div></div><div><div><div></div></div></div><h2><div><div>References</div></div></h2><div><div><div></div></div></div><div><div><div>[1] Salovey, Peter, and John D. Mayer. 1990. "Emotional intelligence." Imagination, cognition and personality9, no. 3: 185-211.</div></div></div><div><div><div></div></div></div><div><div><div>[2] Grimm, Michael, Emily Mower, Kristian Kroschel, and Shrikanth Narayanan. 2006. "Combining categorical and primitives-based emotion recognition." In Signal Processing Conference, 2006 14th European, pp. 1-5. IEEE.</div></div></div><div><div><div></div></div></div><div><div><div>[3] Alm, Cecilia Ovesdotter, and Richard Sproat. 2005. "Perceptions of emotions in expressive storytelling." In Ninth European Conference on Speech Communication and Technology.</div></div></div><div><div><div></div></div></div><div><div><div>[4] Batliner, Anton, Kerstin Fischer, Richard Huber, Jörg Spilker, and Elmar Nöth. 2003. "How to find trouble in communication." Speech communication 40, no. 1-2: 117-143.</div></div></div><div><div><div></div></div></div><div><div><div>[5] Ericsson, Karl Anders, and Herbert Alexander Simon. 1993. Protocol analysis: Verbal reports as data, Rev. the MIT Press.</div></div></div><div><div><div></div></div></div><div><div><div>[6] Hoque, Mohammed E., Mohammed Yeasin, and Max M. Louwerse. 2006. "Robust recognition of emotion from speech." In International Workshop on Intelligent Virtual Agents, pp. 42-53. Springer, Berlin, Heidelberg.</div></div></div><div><div><div></div></div></div><div><div><div>[7] Cohn, Jeffrey F., and Takeo Kanade. 2007. "Use of automated facial image analysis for measurement of emotion expression." Handbook of emotion elicitation and assessment: 222-238.</div></div></div><div><div><div></div></div></div><div><div><div>[8] Craig, Scotty, Sidney D'Mello, Barry Gholson, Amy Witherspoon, Jeremiah Sullins, and Arthur Graesser. 2004. "Emotions during learning: The first steps toward an affect sensitive intelligent tutoring system." In E-Learn: World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education, pp. 264-268. Association for the Advancement of Computing in Education (AACE).</div></div></div><div><div><div></div></div></div><div><div><div>[9] Hudlicka, Eva, and Michael D. Mcneese. 2002. "Assessment of user affective and belief states for interface adaptation: Application to an Air Force pilot task." User Modeling and User-Adapted Interaction 12, no. 1: 1-47.</div></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="15" id="mark-67f608f2-48ee-48d4-b6d2-19d81a4580e5" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div>[10] Spisak, April. 2008. "Drawing Words &amp; Writing Pictures: Making Graphics: Manga, Graphic Novels, and Beyond." Bulletin of the Center for Children’s Books 62, no. 2: 62-63.</div></div></div><div><div><div></div></div></div><div><div><div>[11] Amheim, Rudolf. 1974. "Art and Visual Perception, a psychology of the creative eye. The New Version."</div></div></div><div><div><div></div></div></div><div><div><div>[12] Barber, John. 2005. "The phenomenon of multiple dialectics in graphics layout." Retrieved October 16 (2002).</div></div></div><div><div><div></div></div></div><div><div><div>[13] Cohn, Neil. 2013. "Visual narrative structure." Cognitive science 37, no. 3: 413-452.</div></div></div><div><div><div></div></div></div><div><div><div>[14] Cohn, Neil, Philip Holcomb, Ray Jackendoff, and Gina Kuperberg. 2012. "Segmenting visual narratives: evidence for constituent structure in graphics." In Proceedings of the Annual Meeting of the Cognitive Science Society, vol. 34, no. 34.</div></div></div><div><div><div></div></div></div><div><div><div>[15] Dobel, Christian, Gil Diesendruck, and Jens Bölte. 2007. "How writing system and age influence spatial representations of actions: A developmental, cross-linguistic study." Psychological Science 18, no. 6: 487-491.</div></div></div><div><div><div></div></div></div><div><div><div>[16] Goodbrey, Daniel. 2015. "The Sound of Digital Graphics." Writing Visual Culture.</div></div></div><div><div><div></div></div></div><div><div><div>[17] Drucker, Johanna. 2008. "Graphic devices: narration and navigation." Narrative 16, no. 2: 121-139.</div></div></div><div><div><div></div></div></div><div><div><div>[18] Holmqvist, Kenneth, and Constanze Wartenberg. 2005. "The role of local design factors for newspaper reading behaviour-an eye-tracking perspective." Lund University Cognitive Studies 127: 1-21.</div></div></div><div><div><div></div></div></div><div><div><div>[19] Johnston, Ollie, and Frank Thomas. 1981. The illusion of life: Disney animation. New York: Disney Editions.</div></div></div><div><div><div></div></div></div><div><div><div>[20] Lu, Dengsheng, and Qihao Weng. 2007. "A survey of image classification methods and techniques for improving classification performance." International journal of Remote sensing 28, no. 5: 823- 870.</div></div></div><div><div><div></div></div></div><div><div><div>[21] Quattoni, Ariadna, Michael Collins, and Trevor Darrell. 2009. "Transfer learning algorithms for image classification." PhD diss., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science.</div></div></div><div><div><div></div></div></div><div><div><div>[22] Quattoni, Ariadna J. 2005. "Object recognition with latent conditional random fields." PhD diss., Massachusetts Institute of Technology.</div></div></div><div><div><div></div></div></div><div><div><div>[23] Guo, Tianmei, Jiwen Dong, Henjian Li, and Yunxing Gao. 2017. "Simple convolutional neural network on image classification." In Big Data Analysis (ICBDA), 2017 IEEE 2nd International Conference on, pp. 721-724. IEEE.</div></div></div><div><div><div></div></div></div><div><div><div>[24] Ciregan, Dan, Ueli Meier, and Jürgen Schmidhuber. 2012. "Multi-column deep neural networks for image classification." In Computer vision and pattern recognition (CVPR), 2012 IEEE conference on, pp. 3642-3649. IEEE.</div></div></div><div><div><div></div></div></div><div><div><div>[25] Rawat, Waseem, and Zenghui Wang. 2017. "Deep convolutional neural networks for image classification: A comprehensive review." Neural computation 29, no. 9: 2352-2449.</div></div></div><div><div><div></div></div></div><div><div><div>[26] Yang, Yang, Zi Huang, Heng Tao Shen, and Xiaofang Zhou. 2011. "Mining multi-tag association for image tagging." World Wide Web 14, no. 2: 133-156.</div></div></div><div><div><div></div></div></div><div><div><div>[27] You, Quanzeng, Jiebo Luo, Hailin Jin, and Jianchao Yang. 2015."Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks." In AAAI, pp. 381-388.</div></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="16" id="mark-344be6b7-9d6f-45e3-9aa6-e496677e5f29" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div>[28] Yuan, Jianbo, Sean Mcdonough, Quanzeng You, and Jiebo Luo. 2013. "Sentribute: image sentiment analysis from a mid-level perspective." In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="25" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ss"><mjx-c class="mjx-c1D5C9 TEX-SS"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ss"><mjx-c class="mjx-c1D5A0 TEX-SS"></mjx-c><mjx-c class="mjx-c1D5A2 TEX-SS"></mjx-c><mjx-c class="mjx-c1D5AC TEX-SS"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="sans-serif">p</mi></mrow><mo>.</mo><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mo>.</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="sans-serif">ACM</mi></mrow></mrow><mo>.</mo></math></mjx-assistive-mml></mjx-container></div></div></div><div><div><div></div></div></div><div><div><div>[29] Cutting, Doug, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. "A practical part-of-speech tagger." In Proceedings of the third conference on Applied natural language processing, pp. 133- 140. Association for Computational Linguistics.</div></div></div><div><div><div></div></div></div><div><div><div>[30] Pantic, Maja, and Leon JM Rothkrantz. 2003. "Toward an affect-sensitive multimodal human-computer interaction." Proceedings of the IEEE 91, no. 9: 1370-1390.</div></div></div><div><div><div></div></div></div><div><div><div>[31] Shafran, Izhak, and Mehryar Mohri. 2005. "A comparison of classifiers for detecting emotion from speech." In Acoustics, Speech, and Signal Processing, 2005. Proceedings.(ICASSP’05). IEEE International Conference on, vol. 1, pp. I-341. IEEE.</div></div></div><div><div><div></div></div></div><div><div><div>[32] Finney, Nathaniel, and Jordi Janer. 2010. "Soundscape generation for virtual environments using community-provided audio databases." In W3C Workshop: Augmented Reality on the Web. Barcelona.</div></div></div><div><div><div></div></div></div><div><div><div>[33] Gong, Yunchao, Yangqing Jia, Thomas Leung, Alexander Toshev, and Sergey Ioffe. 2013. "Deep convolutional ranking for multilabel image annotation." arXiv preprint arXiv:1312.4894</div></div></div><div><div><div></div></div></div><div><div><div>[34] Geng, Yanyan, Guohui Zhang, Weizhi Li, Yi Gu, Ru-Ze Liang, Gaoyuan Liang, Jingbin Wang, Yanbin Wu, Nitin Patil, and Jing-Yan Wang. 2017. "A novel image tag completion method based on convolutional neural network." arXiv preprint arXiv:1703.00586.</div></div></div><div><div><div></div></div></div><div><div><div>[35] Kupiec, Julian. 1992. "Robust part-of-speech tagging using a hidden Markov model." Computer Speech &amp; Language 6, no. 3: 225-242.</div></div></div><div><div><div></div></div></div><div><div><div>[36] Owoputi, Olutobi, Brendan O'Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. "Improved part-of-speech tagging for online conversational text with word clusters." Association for Computational Linguistics.</div></div></div><div><div><div></div></div></div><div><div><div>[37] Charniak, Eugene, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. "Equations for part-of-speech tagging." In AAAI, vol. 93, pp. 784-789.</div></div></div><div><div><div></div></div></div><div><div><div>[38] Wu, Jiajun, Yinan Yu, Chang Huang, and Kai Yu. 2015. "Deep multiple instance learning for image classification and auto-annotation." In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 3460-3469. IEEE.</div></div></div><div><div><div></div></div></div><div><div><div>[39] WordNet Database. Accessed from URL https://wordnet.princeton.edu/wordnet/frequently-asked-questions/database/as on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="26" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msup><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c62"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>13</mn></mrow><mrow data-mjx-texclass="ORD"><mtext>th&nbsp;</mtext></mrow></msup><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Feb</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2018</mn></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div><div><div><div></div></div></div><div><div><div>[40] Feuz, K. D., &amp; Cook, D. J. (2015). Transfer learning across feature-rich heterogeneous feature spaces via feature-space remapping (FSR). ACM Transactions on Intelligent Systems and Technology (TIST), 6(1), 3.</div></div></div><div><div><div></div></div></div><div><div><div>[41] Chattopadhyay, R., Sun, Q., Fan, W., Davidson, I., Panchanathan, S., &amp; Ye, J. (2012). Multisource domain adaptation and its application to early detection of fatigue. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(4), 18.</div></div></div><div><div><div></div></div></div><div><div><div>[42] Chen, Danqi, and Christopher Manning. 2014. "A fast and accurate dependency parser using neural networks." In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 740-750.</div></div></div><div><div><div></div></div></div><div><div><div></div></div></div></span></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div data-page="17" id="mark-c7ebcb36-c112-42af-a1f1-439d0f625da8" class="markdown-parser-view mb-5 relative cursor-pointer"><div><span style="display: block;"><div><div><div>[43] Manning, Christopher, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. "The Stanford CoreNLP natural language processing toolkit." In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="27" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="OP"><mjx-mi class="mjx-n"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c70"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="OP"><mi>pp</mi><mo>.</mo></mrow></math></mjx-assistive-mml></mjx-container> 55-60.</div></div></div><div><div><div></div></div></div><div><div><div>[44] Yu Hye-Yeon, and Kim Moon-Hyun, "Automatic Event Extraction methd for Analysing Text Narrative Structure", 2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM), 2021, pp. 1-4.</div></div></div></span></div></div></div></div>
      </body>
    </html>
  