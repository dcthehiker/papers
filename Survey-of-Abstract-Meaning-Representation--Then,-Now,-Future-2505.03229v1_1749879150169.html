
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <!-- Meanless: のことでご了し、コンセーーンのからということにも-->
<h1>Survey of Abstract Meaning Representation: Then, Now, Future</h1>
<p>BEHEROOZ MANSOURI, AIIR Lab, University of Southern Maine, USA</p>
<p>This paper presents a survey of Abstract Meaning Representation (AMR), a semantic representation framework that captures the meaning of sentences through a graph-based structure. AMR represents sentences as rooted, directed acyclic graphs, where nodes correspond to concepts and edges denote relationships, effectively encoding the meaning of complex sentences. This survey investigates AMR and its extensions, focusing on AMR capabilities. It then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by showing traditional, current, and possible futures approaches. It also reviews various applications of AMR including text generation, text classification, and information extraction and information seeking. By analyzing recent developments and challenges in the field, this survey provides insights into future directions for research and the potential impact of AMR on enhancing machine understanding of human language.</p>
<p>Additional Key Words and Phrases: Semantic Representation, Semantic Parsing, Abstract Meaning Representation</p>
<h2>1 INTRODUCTION</h2>
<p>Semantic representation refers to the structured modeling of natural language meaning, aiming to capture the underlying semantics of linguistic expressions independently of specific words or syntactic structures. This capability is essential for advancing computational linguistics, natural language processing, and our broader understanding of human language. Various frameworks have been proposed to represent meaning, each developed to capture different aspects of semantics and facilitate diverse downstream applications [2, 83, 92].</p>
<p>Central to meaning representation is the concept of events, which serve as the primary units of semantic content. Events are typically expressed through predicates (e.g., verbs) that define the core action or state. These predicates are accompanied by arguments representing participants, properties, or modifiers of the event (e.g., location or time), and are connected by semantic relations. Such structures enable the representation of complex sentence meanings in a concise and formalized manner. As an example, in the sentence "Joe saw the dog", see is the event, with Joe and dog as the arguments, representing the subject and the object of the event.</p>
<p>Meaning representations can be categorized as deep or shallow [92]. Shallow frameworks, such as Semantic Roles [34] and Universal Decompositional Semantics (UDS) [116], capture the surface-level semantics of text, often focusing on high-level relations between predicates and their arguments. In contrast, deep frameworks go further, representing sub-events and decomposing arguments into finer-grained components. Deep representations are designed to model a wide range of phenomena, including negations, temporal dependencies, causal relationships, comparisons, and modifiers, thus offering a more comprehensive understanding of textual meaning.</p>
<p>Among the deep semantic frameworks, Abstract Meaning Representation (AMR) stands out for its ability to encode the meaning of an entire sentence in a unified, graphical form. Unlike logic-based or distributional approaches, AMR emphasizes semantic abstraction, collapsing syntactic variability into a canonical structure that facilitates high-level reasoning. Originally introduced by Langkilde and Knight (1998) [53] as part of the Nitrogen language generation system, AMR was conceived as a labeled directed graph derived from the PENMAN Sentence Plan Language [63]. Early AMRs employed concepts from the SENSUS knowledge base and represented meanings using a simple "label/concept" notation. While this early work was primarily explored in the context of language generation, interest in AMR expanded noticeably after Banarescu et al. revisited it for semantic banking (SemBanking) in 2013 [7].</p>
<!-- Meanless: Author's address: Beherooz Mansouri, behrooz.mansouri@maine.edu, AIIR Lab, University of Southern Maine, Portland, Maine, USA. 1--><!-- Meanless: 2 Behrooz Mansouri-->
<!-- Media -->
<!-- figureText: The boy desires the girl to believe him. (d / desire-01 desire-01 :ARG0 (b / boy) :ARG0 :ARG1 :ARG1 (b2 / believe-01 :ARG1 :ARG0 (g / girl) boy believe-01 :ARG1 b)) :ARG0 girl (B) AMR with PennMan Notation (C) AMR Graph The boy desires to be believed by the girl. The boy has a desire to be believed by the girl. The boy's desire is for the girl to believe him. The boy is desirous of the girl believing him. (A) Input Sentences -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_1.jpg?x=311&#x26;y=266&#x26;w=1281&#x26;h=342&#x26;r=0">
<p>Fig. 1. Abstract Meaning Representation for five different input sentences (A) with the same semantics. (B) shows the AMR with PennMan notation and (C) represents the corresponding graph.</p>
<!-- Media -->
<p>In this survey, we present an overview of Abstract Meaning Representation. We begin by introducing AMR and explaining its fundamental concepts and structures. Next, we review approaches for AMR parsing (generating AMRs from text) and AMR-to-text generation (transforming AMRs back into text). We then explore the adaptation of AMR for non-English languages and approaches for multilingual AMR. Finally, we discuss downstream applications of AMR, showing its utility in areas such as text generation, information extraction, information seeking, and text classification.</p>
<h2>2 ABSTRACT MEANING REPRESENTATION</h2>
<p>Abstract Meaning Representation captures "who is doing what to whom" in a sentence. Each sentence is represented as a rooted,directed,acyclic graph with nodes representing concepts and edges showing the relations. \({}^{1}\) AMR uses PropBank (Proposition Bank) [78] frames. PropBank is a layer of annotation added on top of a Treebank that captures the semantic roles of the arguments in a sentence. A Treebank is a corpus of sentences annotated with syntactic structure, typically in the form of parse trees. For example, the Penn Treebank annotates English sentences with part-of-speech tags and phrase structure trees. These trees represent the syntactic organization of sentences, showing how words combine into phrases (like noun phrases and verb phrases) and how these phrases combine into complete sentences. PropBank annotates sentences with predicate-argument structures, indicating which participants in the sentence fill specific roles in relation to a verb (or predicate). These roles are based on the idea of thematic roles like agent (the doer of the action), patient (the entity affected by the action), and location (where the action takes place).</p>
<p>PropBank is an important bridge between syntax (as represented in Treebanks) and semantics because it links verb predicates to their roles, enabling understanding of who is doing what to whom in a sentence. Considering the sentence "The cat sat on the mat" as an example, the PropBank annotation is Arg0: The cat (agent) and ArgM-LOC: on the mat (location). PropBank provides the predicate 'sit.01' for the verb 'sit' and defines the agent (the cat) and location (on the mat). Abstract Meaning Representation goes further than PropBank by capturing the complete semantics of a sentence in a graph structure. AMR abstracts from syntactic details and focuses entirely on the meaning of the sentence. It not only represents the predicate-argument structure (as PropBank does), but also includes additional semantic information such as negation, modality, quantification, and relations between multiple predicates.</p>
<p>AMR represents sentences as graphs where nodes are concepts (actions, entities, etc.), and edges are relationships between those concepts. AMR also normalizes certain linguistic variations, meaning different syntactic constructions (e.g., active/passive) that convey the same meaning will have the same AMR. Fig. 1 shows five different sentences that are parsed to a unique AMR. \({}^{2}\) In this AMR,’desire-01’ has the suffix ’-01’ representing the specific sense of the verb 'desire' based on PropBank; want, crave, desire. The ARG0 connects the verb 'desire' to its subject, 'boy', and ARG1 connects this verb to 'believe', its proposition. The slash (/) in PennMan notation is shorthand for the instance relation. The nodes in this graph show concepts. The concept 'b/ boy' refers to instance 'boy', and is called 'b'. This helps with showing graphs re-entrancy in PennMan notation.</p>
<hr>
<!-- Footnote -->
<p>\({}^{1}\) <a href="https://github.com/amrisi/amr-guidelines/blob/master/amr.md">https://github.com/amrisi/amr-guidelines/blob/master/amr.md</a></p>
<!-- Footnote -->
<hr><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 3-->
<p>The key features of AMR are:</p>
<p>(1) Graph-Based Representation: AMR represents sentences as rooted, directed, acyclic graphs (DAGs). The graph nodes correspond to concepts (usually derived from the words in the sentence), and the edges represent the relationships between those concepts. This makes AMR particularly expressive, and capable of representing complex syntactic structures like coordination, subordination, and coreference. Graphical editors such as metAMoRphosED [38] are designed for creating and modifying AMRs.</p>
<p>(2) Abstract: AMR abstracts away from the surface syntactic structure, meaning that sentences with different word orders or syntactic constructions that convey the same meaning will have the same AMR. This abstraction facilitates better generalization for downstream tasks.</p>
<p>(3) Uniformity: AMR aims to provide a uniform, consistent semantic representation for different linguistic constructions. For example, both active and passive voice sentences have the same AMR as long as they convey the same meaning, regardless of differences in syntactic structure.</p>
<h3>2.1 What does an AMR Capture?</h3>
<p>Over time, AMR conventions and standards have been changed to include more linguistic phenomena and increase their application for downstream tasks. Despite these changes, AMR in its foundational format captures a wide variety of semantic and syntactic features, including:</p>
<ul>
<li>
<p>Frame Arguments: AMR uses PropBank framesets to represent the semantic roles of words in a sentence. For example, the frameset "desire-01" has slots for the wanter (:arg0), the wanted (:arg1).</p>
</li>
<li>
<p>General Semantic Relations: In addition to frame arguments, AMR uses nearly 100 general semantic relations to capture relationships between entities, events, and properties. This includes semantic relations such as time, location, and manner.</p>
</li>
<li>
<p>Coreference: AMR represents coreference by reusing variables to refer to the same entity. In the example from Fig. 1, 'the boy' and 'him' are both represented with variable 'b'.</p>
</li>
<li>
<p>Inverse Relations: AMR uses inverse relations, such as :arg0-of and :quant-of, to create rooted structures representing the focus of the sentence.</p>
</li>
<li>
<p>Modals and Negation: AMR represents negation with the :polarity relation and expresses modals with concepts.</p>
</li>
<li>
<p>Questions: AMR uses the concept amr-unknown to represent questions and the :mode relation to represent yes-no questions, imperatives, and embedded questions.</p>
</li>
<li>
<p>Verbs and Nouns: AMR uses PropBank framesets to represent both verbs and nouns.</p>
</li>
<li>
<p>Adjectives and Prepositions: AMR uses a variety of mechanisms to represent adjectives and prepositions, including PropBank framesets, newly defined framesets, and the :prep-X relation for cases where no appropriate relation exists.</p>
</li>
</ul>
<hr>
<!-- Footnote -->
<p>\({}^{2}\) Generated using SPRING parser.</p>
<!-- Footnote -->
<hr><!-- Meanless: 4 Behrooz Mansouri-->
<!-- Media -->
<p>Table 1. Linguistics aspects that cannot be captured by AMRs.</p>
<table><tbody><tr><td>Aspect</td><td>Example Sentence(s)</td><td>AMR</td></tr><tr><td>Verb Tense and Aspect</td><td>John eats &#x26; John is eating</td><td>(e / eat-01 :arg0 (j / John))</td></tr><tr><td>Syntactic Structure</td><td>The boy ate the apple &#x26; The apple was eaten by the boy</td><td>(e / eat-01 :arg0 (b / boy) :arg1 (a / apple))</td></tr><tr><td>Ambiguity</td><td>John saw the man with a telescope</td><td>(s / see-01 :arg0 (j / John) :arg1 (m / man) :manner (t / telescope))</td></tr><tr><td>Word Order</td><td>Only John ate the apple &#x26; John only ate the apple</td><td>(e / eat-01 :arg0 (j / John) :arg1 (a / apple))</td></tr><tr><td>Figurative Language</td><td>Kick the bucket</td><td>(k / kick-01 :arg0 (p / person) :arg1 (b / bucket)</td></tr><tr><td>Sentiment and Emotion</td><td>I am ecstatic &#x26; I am happy</td><td>(h / happy-01 :arg0 (i / I))</td></tr><tr><td>Intentionality vs. Factuality</td><td>He wants to go</td><td>(w / want-01 :ARG0 (h / he) :ARG1 (g / go-02 :ARG0 h))</td></tr><tr><td>Semantic Variation</td><td>He was absent &#x26; He was not present</td><td>(a / absent-01 :ARG1 (h / he)) (p / present-02 :polarity - :ARG1 (h / he))</td></tr></tbody></table>
<!-- Media -->
<ul>
<li>
<p>Named Entities: AMR represents named entities with the :name relation and standardized forms for approximately 80 named-entity types.</p>
</li>
<li>
<p>Copula: AMR represents copulas using the :domain relation.</p>
</li>
<li>
<p>Reification: AMR allows relations to be treated as first-class concepts through a process called reification.</p>
</li>
</ul>
<p>Although AMR is a strong suit for the mentioned list, there are some aspects that it cannot capture; this includes the following (with examples shown in Table 1):</p>
<ul>
<li>
<p>Verb Tense and Aspect: AMR does not fully capture verb aspect (progressive, habitual, perfect) or nuanced tense distinctions beyond past, present, or future. For example, "John eats" and "John is eating" are both mapped to the same AMR.</p>
</li>
<li>
<p>Morphological Information: AMR ignores inflectional morphology such as plural vs. singular forms, verb conjugations, and gender markers. For example, both 'cat' and 'cats' are represented as 'cat'.</p>
</li>
<li>
<p>Syntactic Structure: AMR abstracts away from syntax, so it does not capture the detailed syntactic structure of sentences. For example, both sentences "The boy ate the apple" and "The apple was eaten by the boy" will have the same AMR.</p>
</li>
</ul><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 5-->
<ul>
<li>
<p>Ambiguity: AMR generally represents only one interpretation of an ambiguous sentence, missing other valid meanings. For instance, the sentence "John saw the man with a telescope", could mean John used a telescope to see the man, or the man had a telescope. AMR would only capture one interpretation.</p>
</li>
<li>
<p>Word Order: AMR does not preserve word order, so it misses distinctions that rely on the order of words for meaning or emphasis. "Only John ate the apple" vs. "John only ate the apple" conveys different meanings, but both would have the same AMR.</p>
</li>
<li>
<p>Figurative Language: AMR struggles with idioms and metaphors, often either representing them literally or abstracting them away without capturing their full figurative meaning. For example, "Kick the bucket" (meaning "to die") might be represented literally as a person kicking a bucket.</p>
</li>
<li>
<p>Sentiment and Emotion: AMR does not handle subtle emotional states or variations in the intensity of emotions well. It can capture basic emotions, but not fine-grained sentiment polarity. Both sentences "I am ecstatic" and "I am happy" might be represented similarly, though the intensity differs.</p>
</li>
<li>
<p>Intentionality vs. Factuality: AMRs will not distinguish between the real events that have occurred and intentions. For example, for the sentence "He wants to go", in the generated AMR, both verbs 'want' and 'go' have the same status, where 'go' may or may not happen.</p>
</li>
<li>
<p>Semantic Variation: AMRs of two different sentences with the same semantic are not identical. For the sentences "He was absent" and "He was not present" two different AMRs are generated.</p>
</li>
</ul>
<p>Although AMR was initially designed only for the English language, later attempts were made to generate AMRs for other languages (see Section 5). AMRs should also be used carefully for syntax other than natural language. For example,for mathematical formulas,AMRs can be generated for simple equations such as \(a = b\) ,but the AMR notations are not well-defined for more complex formulas [65].</p>
<h3>2.2 AMR Annotations</h3>
<p>The first foundational work that introduced annotation guidelines for AMR was introduced in 2013 [7]. This work defined the key principles of AMR; how to abstract away from syntax, the use of semantic roles (inspired by PropBank), and the representation of core concepts and their relationships as graphs. AMRs were manually generated for \(\sim  {5.6}\mathrm{\;K}\) sentences, including the novel The Little Prince. With this, the theoretical framework and methodology for creating an AMR corpus was introduced, which became the foundation for the Linguistic Data Consortium (LDC) AMR datasets.</p>
<p>The LDC has released three versions of the AMR annotations, each consisting of thousands of English sentences annotated with AMRs, along with updates and improvements in subsequent releases. These datasets are:</p>
<p>(1) LDC2014T12 \({}^{3}\) (AMR 1.0) [79]: As a starting point,this dataset contains \(\sim\) 13K English natural language sentences from newswires, weblogs, and web discussion forums. The ontology used in AMR 1.0 was less detailed and comprehensive compared to later versions.</p>
<p>(2) LDC2017T10 \({}^{4}\) (AMR 2.0) [80]: This version provides more precise and consistent guidelines for annotating AMRs, supporting the representation of a wider range of semantic phenomena, such as temporal expressions. It includes \(\sim  {39}\mathrm{\;K}\) English natural language sentences from broadcast conversations,newswire,weblogs,and web discussion forums.</p>
<hr>
<!-- Footnote -->
<p>\({}^{3}\) <a href="https://catalog.ldc.upenn.edu/LDC2014T12">https://catalog.ldc.upenn.edu/LDC2014T12</a></p>
<p>\({}^{4}\) <a href="https://catalog.ldc.upenn.edu/LDC2017T10">https://catalog.ldc.upenn.edu/LDC2017T10</a></p>
<!-- Footnote -->
<hr><!-- Meanless: 6 Behrooz Mansouri-->
<p>(3) LDC2020T02 \({}^{5}\) (AMR 3.0) [51]: This is the most comprehensive,with the largest number of sentences and the most refined and consistent annotations. It includes \(\sim  {59}\mathrm{\;K}\) English natural language sentences from broadcast conversations, newswires, weblogs, web discussion forums, fiction, and web text.</p>
<h3>2.3 AMR Enrichment</h3>
<p>While AMR annotations have been updated throughout time, several approaches have been proposed to enrich the AMRs to cover more semantics and meaning. As AMR fails to capture time and aspect, Donatelli et al. [25] proposed an annotation scheme to support concepts for time such as past, now, future, and concepts for aspects including stable, ongoing,and complete. Later,approaches explored automating the enrichment of AMRs. For example, \({A}^{3}\left\lbrack  {46}\right\rbrack\) ,enriches AMRs to better represent numbers, (in)definite articles, quantification determiners, and intentional arguments. To solve the issue of intentionality versus factuality, a new role :content has been added to AMRs [117], which is interpreted as an intentional operator to represent the scope of modal predicates such as attitude verbs.</p>
<p>These graphs are enriched with additional data from the domain to use AMRs for domain-specific applications, mostly in a format similar to AMRs. One approach for enrichment is adding new roles to AMRs. For human-robot interaction, Dialogue-AMR [13], developed an inventory of speech acts that represents not only the content of an utterance but the illocutionary force behind it, as well as tense and aspect. Several speech act relations were added to this representation that cannot be directly reflected in AMR. For example, for speech act 'Offer', the relation 'Offer-SA' is considered,meaning \(S\) is committed to the feasibility of the plan of action,and \(A\) is obliged to consider action and respond. Similarly, Gesture AMR [15] uses the idea of Dialogue-AMR for generating enriched AMRs that can capture the meaning of gestures by including gesture acts.</p>
<p>Another alternative is to integrate other graphs into AMR and have a unified graph representation of input text with text of a specific domain. For instance, the MathAMR [65] representation combines the AMR for text representation, and integrates operator trees for formulas in the place of formula to have a unified representation of text and formulas. Operator trees have a similar representation as AMRs, showing what operator should be applied to what operands. The edge labels in operator trees show the order of operands and for non-cumulative operators such as addition, the edge labels for operands are the same (with value 0 ). Fig. 2 shows MathAMR for input text "Find \({x}^{n} + {y}^{n} + {z}^{n}\) general solution". First, in (A), the AMR is generated for the modified text, where a placeholder replaces the formula. Then, the operator tree representation of the formula (in (B)), replaces the AMR node representing the formula, shown in (C).</p>
<h2>3 TEXT-TO-AMR (PARSING TASK)</h2>
<p>So far, we discussed what AMRs are and how they are annotated. A central challenge, however, is the automatic generation of AMRs for a large volume of natural language text. AMR Parsers are designed to generate AMR graphs for the input natural language text. Words considered semantically light ("do not carry meaning"), such as function words, are typically excluded from the AMR of a sentence. For effective automatic AMR parsing, it is crucial to explicitly represent the alignment between word tokens in the input sentence and the corresponding concepts and relations within its AMR graph. This alignment provides a direct correspondence between the text and its semantic representation.</p>
<p>Early AMR parsing approaches were predominantly alignment-based and employed transition systems to generate AMR graphs. With advancements in neural networks, subsequent approaches, often based on sequence-to-sequence models and graph neural networks, became prevalent. More recently, the application of large language models to AMR parsing has been explored, and future research is likely to investigate their potential more deeply. Before reviewing specific parsing methodologies, it is essential to understand how AMR parsers are evaluated. Therefore, this section will first review the evaluation metrics used for AMR parsers. Then we will review past, current, and possible future approaches for AMR parsing.</p>
<hr>
<!-- Footnote -->
<p>\({}^{5}\) <a href="https://catalog.ldc.upenn.edu/LDC2020T02">https://catalog.ldc.upenn.edu/LDC2020T02</a></p>
<!-- Footnote -->
<hr><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 7-->
<!-- Media -->
<!-- figureText: U!plus find-01 :arg(   ) :mode (O!SUP you thing imperative ( V!z :mod :arg1-of solve-01 general-02 equal-0 / :arq2 \\installating Math U!plus \( ) \) :op0 O!SUP (orsup) (0!SUP) V!n (V!y V!n \( \mathrm{V}!\mathrm{z} \) \( \mathrm{V} \) ! \( \mathrm{n} \) (C) MathAMR :arg0 :mode (orsup) (0!SUP) you thing imperative :arg1-of :mod V!x V!n solve-01 general-02 equal-01 /:arq2 :math Math (A) Abstract Meaning Representation (B) Operator Tree -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_6.jpg?x=207&#x26;y=270&#x26;w=1278&#x26;h=539&#x26;r=0">
<p>Fig. 2. MathAMR [65] for input sentence "Find \({x}^{n} + {y}^{n} + {z}^{n}\) general solution". (A) AMR is generated for the input text,with the formula replaced with a placeholder (PL) for the formula. Then the operator tree representation of the formula (B), is integrated into the AMR, replacing the placeholder in AMR with the root of the operator tree, resulting in MathAMR shown in (C).</p>
<!-- Media -->
<h3>3.1 Evaluation Metrics</h3>
<p>Evaluating the performance of AMR parsers is crucial for assessing their ability to generate AMR graphs from natural language text accurately. One of the earliest reference-based metrics developed for this task is SMatch [17], which quantifies the similarity between two AMRs by aligning their nodes and counting the number of matching graph triples. These triples are categorized into two types:</p>
<ul>
<li>\(&#x3C; {v}_{1}\) ,rel,val \(>\) ,where val represents a node value (e.g.,(d,instance,chase-01))</li>
</ul>
<p>\(\bullet   &#x3C; {v}_{1},{rel},{v}_{2} >\) ,where both \({v}_{1},{v}_{2}\) are variables (e.g.,(d,ARG0,a))</p>
<p>While both precision and recall can be calculated using SMatch, most studies report the F1-score as the primary evaluation metric. To illustrate, Table 2 displays the PENMAN representations and corresponding triples for the gold standard and predicted AMRs of the sentences "The dog chased the ball" and "The canine chased the toy" respectively. Precision is computed as the ratio of matched triples to the total number of triples in the predicted AMR (e.g., \(\frac{3}{5} = {0.6}\) ). Recall is calculated as the ratio of matched triples to the total number of triples in the gold standard AMR (e.g., \(\frac{3}{5} = {0.6}\) ). The SMatch score is then the harmonic mean of precision and recall, which is 0.6 in this example.</p>
<p>While SMatch is commonly used in research, it exhibits several limitations. Its reliance on a greedy hill-climbing algorithm for one-to-one node mapping can compromise robustness. Moreover, structural matching alone is insufficient for capturing semantic similarity, and SMatch provides only a single aggregate score, offering little insight into which specific aspects of the parsing may have failed. To address these issues, a fine-grained variant of SMatch was proposed to assess agreement across dimensions such as polarity, semantic roles, and coreference [23]. Subsequently, variants such as \({\mathrm{S}}^{2}\) Match [75] and SMatch++ [73] were introduced. \({\mathrm{S}}^{2}\) Match enables soft semantic matching,relaxing the requirement for exact triple matches, while SMatch++ incorporates three key modifications: 1) Preprocessing steps (e.g., removing duplicate triples), 2) Enhanced alignment through lossless compression, and 3) Scoring mechanisms that consider subgraphs and fine-grained matching. Another related metric, SEMA [3], uses a breadth-first search approach to compute a maximum score, resulting in a deterministic result and offering a reproducible alternative to SMatch's greedy approach, with a stricter matching.</p><!-- Meanless: 8 Behrooz Mansouri-->
<!-- Media -->
<p>Table 2. AMRs and Triples for a Gold sentence "The dog chased the ball". The precision and recall of SMatch are both 0.6, leading to an F1-Score of 0.6 .</p>
<table><tbody><tr><td colspan="2">AMR</td><td colspan="2">Triples</td></tr><tr><td>Gold Standard</td><td>Predicted</td><td>Gold Standard</td><td>Predicted</td></tr><tr><td></td><td></td><td>(d, instance, chase-01)</td><td>(d, instance, chase-01)</td></tr><tr><td>(d / chase-01</td><td>(d / chase-01</td><td>(a, instance, dog),</td><td>(a, instance, canine)</td></tr><tr><td>:ARG0 (a / dog)</td><td>:ARG0 (a / canine)</td><td>(b, instance, ball)</td><td>(b, instance, toy)</td></tr><tr><td>:ARG1 (b / ball))</td><td>:ARG1 (b / toy))</td><td>(d, ARG0, a)</td><td>(d, ARG0, a)</td></tr><tr><td></td><td></td><td>(d, ARG1, b)</td><td>(d, ARG1, b)</td></tr></tbody></table>
<!-- Media -->
<p>While SMatch and its variants are the most widely used metrics to evaluate AMR parsers, a set of alternative metrics has also been proposed. SemBlue [95] is a metric based on the BLEU score that linearizes each AMR using breadth-first traversal, treating nodes as unigrams and connected node-relation pairs as bigrams. Opitz et al. [75] conducted a comparative analysis of SemBleu and SMatch based on fundamental evaluation principles, including the requirement that only semantically equivalent graphs should achieve the maximum score, and the principle of symmetry. Their analysis revealed that SemBleu, in contrast to SMatch, violates some of these principles. The Granular AMR Parsing Evaluation Suite (GrAPES) [35] addresses the limitations of SMatch by providing a more granular evaluation. GrAPES evaluates parsers across a diverse spectrum of linguistic phenomena, such as generalization to unseen labels and structures, and coreference resolution, utilizing a combination of established and newly created sentence-AMR pairs.</p>
<p>Beyond string-based metrics, embedding models have also been explored for AMR parsing evaluation. Opitz et al. [74] utilized the Wasserstein-Weisfeiler Lehman kernel (WWLK) to transform AMR graphs into high-dimensional vector representations. Their approach involves iteratively propagating node embeddings by incorporating contextual information and subsequently employing WWLK to calculate the minimum cost of graph transformation. WWLK, in its original formulation, considers only node embeddings (GloVe embeddings) within AMR graphs, ignoring edge labels. Therefore, \({\mathrm{{WWLK}}}_{\theta }\) was extended to incorporate AMR edge labels learning,which requires additional training data. AMRSim [94] is another recent metric based on Graph Neural Networks (GNN) that adopts the pre-trained language model BERT as the backbone and incorporates GraphNN adapters to capture the structural information of AMR graphs.</p>
<h3>3.2 Then: Alignment and Transition-based Parsing</h3>
<p>In the early days of AMR parsing, alignment-based and transition-based approaches were the predominant strategies. Although these methods were initially proposed as the main strategies for AMR parsing, subsequent models have also been developed from these foundational approaches, including joint alignment-parsing models and neural transition parsers that integrate alignment learning. Alignment-based methods first align spans of the input text to AMR nodes using rule-based heuristics and then perform concept identification and relation extraction based on these alignments. In contrast, transition-based approaches generate AMRs incrementally by predicting a sequence of actions (e.g., shift, reduce, or arc operations) that build the graph structure, often guided by an intermediate dependency tree. Fig. 3 illustrates the core ideas behind these approaches.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 9-->
<!-- Media -->
<!-- figureText: nsubj went ARGO go-02 ARG4 boy home Alignment-based The boy went home advmod the home Transition-based det punct boy -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_8.jpg?x=330&#x26;y=274&#x26;w=1029&#x26;h=306&#x26;r=0">
<p>Fig. 3. Transition-based and Alignment-based approaches for AMR Parsing. Transition-based methods incrementally generate AMRs through a sequence of actions, often guided by a dependency tree (Left). Alignment-based methods first align text spans to AMR nodes and then build the graph based on these alignments (Right).</p>
<!-- Media -->
<p>JAMR [32] is widely recognized as the first AMR parser that operates in a two-stage pipeline. First, it maps the concepts in the sentence to concept nodes in the graph using a rule-based semi-Markov model for sequence labeling. Then, it determines the relationships between these concepts by finding the highest-scoring connected subgraph. To find this subgraph, JAMR uses a Maximum Preserving, Simple, Spanning, Connected Subgraph (MSCG) algorithm, which is a novel modification of Kruskal's minimum spanning tree algorithm. This algorithm constructs an initial subgraph that satisfies the preserving, simple, and connected constraints. If the resulting subgraph violates the deterministic constraint, a technique called Lagrangian Relaxation is employed to iteratively adjust the edge scores to enforce this constraint.</p>
<p>Early transition-based parsers initially constructed a dependency tree from the input text and then used a transition system to convert this tree into an AMR graph [109]. These approaches generate the graph incrementally, resulting in higher graph validity. For instance, AMR-EAGER [23] processes an input sentence from left to right, in a manner similar to transition-based dependency parsers. This parser has three main components: a stack, which holds the nodes of the AMR graph that have been partially constructed; a buffer, which contains the indices of the word tokens from the input sentence that are yet to be processed; and a set of edges, which represents the AMR relationships that have been established so far. The parser starts with an initial configuration and applies a sequence of transition actions (Shift, LArc, RArc, Reduce) until it reaches a terminal configuration, where the entire sentence has been processed and the AMR graph is complete.</p>
<p>CAMR parser [108] is another transition-based approach to AMR parsing. However, unlike AMR-EAGER, CAMR operates by transforming a dependency parse tree of an input sentence into its corresponding AMR graph. CAMR uses a set of transformation actions performed when processing nodes and edges, as well as actions to infer concepts that do not directly correspond to any word in the sentence.</p>
<h3>3.3 Now: Neural AMR Parsing and BART</h3>
<p>With the advances in neural network applications, AMR parsing has been explored using these models. Common approaches treat this task either as a sequence-to-sequence problem or as a graph prediction problem. Fig. 4 illustrates these approaches: (A) seq-to-seq models generate a linearized AMR (commonly via depth-first search), and (B) graph prediction models directly predict the AMR graph from the input text. In both approaches, pre-processing steps serve as a solution for data sparsity, with a set of post-processing steps for AMR validation.</p><!-- Meanless: 10 Behrooz Mansouri-->
<!-- Media -->
<!-- figureText: Post-Processing go-02 The boy went home Post-Processing boy home (B) Graph Prediction AMR Parsing go-02 ARG0 boy ARG4 home Encoder Decoder The boy went home (A) Seq-to-seq AMR Parsing -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_9.jpg?x=440&#x26;y=269&#x26;w=1027&#x26;h=296&#x26;r=0">
<p>Fig. 4. Overview of current AMR parsing approaches. (A) Seq-to-seq models generate a linearized AMR, and (B) Graph prediction directly predicts AMR nodes and edges. Both approaches typically involve pre- and post-processing steps.</p>
<!-- Media -->
<p>Peng et al. [84] were among the first to explore seq-to-seq models for AMR parsing. The architecture consists of an LSTM-based encoder-decoder framework enhanced with an attention mechanism where the encoder processes the input sentence and a decoder generates the linearized AMR. The encoder utilizes a bidirectional RNN, incorporating a backward RNN to capture information from the reverse order of the input sequence. The decoder, at each step, employs an attention mechanism to compute a weighted sum of the input hidden layers, allowing it to focus on relevant parts of the input sequence for generating the output. Data sparsity was addressed by mapping low-frequency concepts and entity subgraphs to a reduced set of category types (e.g., mapping all date entity subgraphs to the category DATE). Although their categorization step was useful in correctly generating AMRs, the overall approach was less effective than previous models such as CAMR. Similarly, NeuralAMR [52] employs a seq-to-seq architecture using a stacked-LSTM with global attention and an unknown word replacement mechanism. This model applies several preprocessing steps, including categorization (using finer categories than Peng et al.) and anonymization of named entities, dates, and quantifiers to reduce data sparsity. One key innovation in NeuralAMR is paired training procedure using a self-training millions of weakly labeled data, and then fine-tuning with human annotated pairs.</p>
<p>As with many other NLP tasks, the introduction of Transformers led to major improvements in AMR parsing, and transformer-based models have become the most widely used. Among transformer-based models, the BART [54] model is commonly used as an encoder-decoder framework. Pre-trained for denoising English text, BART serves as a strong foundation for AMR parsing. For example, SPRING (Symmetric PaRsIng aNd Generation) [9] directly maps an input sentence to its corresponding AMR graph. It applies depth-first search for graph linearization and expands BART's vocabulary to support AMR tokens; special tokens are used to represent variables in the linearized graph, which helps handle co-referring nodes and prevent data loss. SPRING also incorporates recategorization to reduce vocabulary size. Similarly, the BART architecture was extended in AMRBART [6] via fine-tuning for both AMR-to-text and text-to-AMR tasks. Unlike SPRING, AMRBART pre-trains the BART model on AMR graphs using two denoising approaches; one at the node/edge level and another at the sub-graph level. For joint AMR graph and text pre-training, four tasks are defined in which the model denoises either the graph or the text using the other modality as context, in both original and noisy input modes.</p>
<p>AMR parsing can also be framed as a graph prediction task. The AMR-GP approach [60] jointly predicts both the concept nodes (representing entities and actions) and the relations (edges) between them, while simultaneously handling the alignment between input words and graph nodes. In contrast to traditional methods that treat alignment and parsing as separate steps, this model jointly predicts the entire AMR structure. Similarly, the Sequence-to-Graph Transduction (STOG) model [121] view ARM parsing as a two-stage process of predicting nodes and edges. First, node prediction is performed using an Extended Pointer-Generator Network, and second, edge prediction is carried out with a Deep Biaffine Classifier, with the training objective jointly minimizing the loss over nodes and edges. A key characteristic of STOG is that it is aligner-free, unlike many other AMR parsers that rely on pre-trained aligners for word-concept alignment. During pre-processing, AMR graphs are linearized into sequences, often by converting the graph into a tree through the duplication of nodes with reentrant relations and then performing a traversal like depth-first search. Post-processing is then used to recover the full AMR graph from the predicted sequence by restoring variables, wiki-links, and handling co-referring nodes.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 11-->
<p>The final category of approaches for AMR parsing are hybrid models. Action-Pointer Transition (APT) parser [123] introduces an action-pointer mechanism to leverage the advantages of both transition-based and graph generation-based approaches. The model aligns parsing actions with the input sentence using a cursor-based pointer mechanism. The cursor moves sequentially through the sentence, predicting actions to build the AMR graph incrementally. A pointer network creates edges by referencing previously generated nodes, embedding graph structure directly into the decoding process. A single transformer architecture with self-attention propagates graph information efficiently, modeling both the action generation and the pointer prediction. This approach enables dynamic and interpretable graph construction while maintaining alignment with the input text. Later, the authors introduced StructBART [124] that integrates pre-trained BART with a transition-based system to improve AMR parsing. The proposed transition system for AMR parsing is designed to make the most of pre-trained decoders like BART while simplifying the structure compared to previous methods.</p>
<h3>3.4 Future: Large Language Model Parsers</h3>
<p>Given the success of large language models in various NLP tasks, researchers have begun investigating their ability to handle AMR parsing. Ettinger et al. [27] examined whether models such as GPT-3, ChatGPT, and GPT-4 can generate AMRs under zero-shot and few-shot settings. In the zero-shot configuration, the prompt was set to "Provide an AMR (Abstract Meaning Representation) parse for this sentence," with a system message (for ChatGPT and GPT-4) instructing, "You are an expert linguistic annotator." In the few-shot setting, five example sentences were provided to guide the models. Evaluation focused on two levels: (1) the overall format, including the highest-level nodes and general semantic accuracy, and (2) the finer accuracy within arguments and modifiers. The results indicate that while LLMs can typically generate valid AMRs (with GPT-4 achieving 100% validity), they struggle to produce correct corresponding AMRs in the zero-shot setting. However, when provided with few-shot demonstrations, all evaluated LLMs not only output 100% valid AMRs but also achieve higher accuracy in generating detailed AMR structures.</p>
<p>In addition to training models on (text, AMR) pairs, multitask training approaches are emerging. BiBL (Bidirectional Bayesian Learning) [18] is one such method that employs a single-stage multitask framework, leveraging multiple loss functions (transduction, generation, and reconstruction losses) to simplify training while enhancing performance. Similarly, recent work on incorporating graph structure into learned representations is exemplified by LeakDistill [106]. During training, the model uses word-to-node alignments to build a word-based graph that mirrors the AMR structure. This graph information is then leaked to the model to guide learning. To address the absence of this leaked information during inference, LeakDistill employs a self-knowledge distillation technique, transferring the knowledge from a teacher model (trained with the graph information) to a student model that only has access to the input text. LeakDistill achieves Smatch scores of 86.1 and 84.6 on AMR 2.0 and AMR 3.0, respectively, positioning it as a current state-of-the-art AMR parser.</p><!-- Meanless: 12 Behrooz Mansouri-->
<p>Despite the progress in AMR parsing, the primary focus, along with the development of annotated corpora, has traditionally been at the sentence level. Consequently, the research in AMR parsing, has concentrated on processing individual sentences. Extending the principles of sentence-level AMR to encompass entire documents introduces a set of challenges, including cross-sentential coreferene, computational cost, and lack of consistent standard. To address these challenges, DOCAMR [70] proposes a novel representation that utilizes coref-entity nodes to link coreferent mentions across sentences, while also providing specific rules for handling named entities and pronouns. To tackle the computational cost of evaluating these larger graphs, the paper introduces DOCSMATCH, an optimized version of the Smatch metric that leverages sentence alignments to improve efficiency. Furthermore, DOCAMR includes a coreference subscore to specifically assess the accuracy of cross-sentential links.</p>
<h2>4 AMR-TO-TEXT (GENERATION TASK)</h2>
<p>While parsing text into AMR is one task, generating text from a given AMR has also been widely explored. This task benefits many downstream applications, such as summarization (see Section 6). It can be defined as generating a sentence that conveys the same meaning as the input AMR graph, essentially, recovering the meaning encoded in the AMR in natural language.</p>
<p>A key challenge is that AMR representations often omit certain linguistic aspects of the original text (see Table 1), therefore, regenerating a full and natural sentence may not capture all the missing details. Models for this task are typically evaluated using machine translation metrics such as BLEU [81], CHRF++ [85], and METEOR [8], which assess lexical overlap, n-gram precision, and alignment quality between generated outputs and reference texts.</p>
<p>Early approaches for AMR-to-text generation relied on rule-based and statistical methods. In the neural network era, two main architectures, seq-to-seq and graph-to-seq models, have been employed. While these models continue to be extensively studied, the development of hybrid approaches that combine both sequential and graph-based representations remains an area for future exploration.</p>
<h3>4.1 Then: Rule-based Generation</h3>
<p>Traditional approaches for AMR-to-text generation have primarily relied on statistical and rule-based methods. The first model proposed for this task was introduced as part of the JAMR system [31]. In JAMR, the AMR graph is first converted into a spanning tree using a breadth-first search traversal. This spanning tree is then processed by a tree-to-string transducer, which generates text based on a set of rules derived from Part-of-Speech (POS) tags and alignment information. Fig. 5 provides an overview of this model.</p>
<p>Pourdamghani et al. [86] proposed an alternative approach that first linearizes the AMR graph into a flat structure and then employs a Phrase-Based Machine Translation (PBMT) system to convert this linearized representation into an English sentence. A key component of this methodology was the learning of a function to linearize the tokens of an AMR graph into an order that resembled English, aiming to reduce the amount of reordering or distortion required during the phrase-based machine translation process. The paper introduced and compared several linearization techniques, with the classifier method demonstrating the most promising results. Their system is trained on AMR-sentence pairs using an alignment algorithm that reduces sparsity by dropping certain structural details (e.g., role edges). This approach has been shown to outperform JAMR on standard AMR datasets, although the tuning of multiple feature functions in PBMT adds engineering complexity. The success of this approach showed the potential of leveraging existing machine translation techniques for the task of AMR-to-text generation, provided that the structural differences between the graph-based AMR and the sequence-based input requirements of PBMT systems could be effectively addressed through a process like linearization.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 13-->
<!-- Media -->
<!-- figureText: ✘ ARG1 ride-01 ARGO The girl wants to ride the purple bicycle. purple (b) Transducer Input (c) Generated Text want-01 want-01 ARGO ARGO ARG1 ✘ girl ARGO ride-01 │ girl ARG1 bicycle mod purple (a) Input AMR Graph -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_12.jpg?x=263&#x26;y=276&#x26;w=1158&#x26;h=445&#x26;r=0">
<p>Fig. 5. JAMR [31] approach for text generation from Abstract Meaning Representation. (a) The input AMR graph is processed to remove re-entrancies, yielding a tree structure. (b) This tree is then used by a tree-to-text transducer, which (c) generates the final text output.</p>
<!-- Media -->
<p>Building on these methods, Song et al. [97] introduced a heuristic extraction algorithm that learns synchronous node replacement grammar rules from sentence-AMR pairs. At test time, the learned graph-to-string rules are applied via a graph transducer, which collapses the input AMR into an output sentence based on these rules. This method effectively addresses the inherent ambiguity in AMR representations, where a single AMR may correspond to multiple textual realizations, by leveraging synchronous grammar to generate diverse and contextually appropriate sentences.</p>
<h3>4.2 Now: Neural Generation</h3>
<p>Similar to AMR parsing, recent approaches for AMR-to-text generation have shifted toward neural architectures, mainly adopting two paradigms: seq-to-seq and graph-to-seq models. Figure 6 illustrates these two approaches.</p>
<p>The NeuralAMR model [52] was among the first to apply a seq-to-seq framework to the generation task. This model uses a stacked-LSTM encoder-decoder architecture with global attention and an unknown word replacement mechanism. In addition, several preprocessing steps-such as linearization and anonymization of named entities, dates, and quantifiers-are applied to mitigate data sparsity.</p>
<p>Building on this, the Structural Transformer model [125] introduced a structure-aware self-attention mechanism within the transformer framework. As the first transformer-based model for AMR-to-text generation, it leverages linearized AMRs along with explicit edge information to preserve the underlying graph structure. By encoding long-distance relations between concepts via self-attention, it improved performance compared to earlier LSTM-based models. More recently, the SPRING model [9] employs BART within a seq-to-seq setup for generation, while models such as BiBL [18] and AMRBART [6] have unified training for both AMR parsing and text generation tasks, further enhancing their overall utility.</p>
<p>Alternatively, the generation task can be approached as graph-to-seq learning. One of the earliest models in this category is presented by Song et al. [98], which uses a graph LSTM to directly encode the structure of AMR graphs. This approach bypasses the need for linearization by capturing complex non-local semantic relationships directly from the graph. Expanding on this idea, the GraphTransformer model [110] adapts the transformer architecture for AMR-to-text generation. In this model, the encoder learns node representations by aggregating information from neighboring nodes through stacked graph attention layers. Each node is assigned two representations: one for aggregating information from outgoing edges (head representation) and another for incoming edges (tail representation). The encoder's graph attention mechanism effectively preserves edge information without incurring a parameter explosion. To further refine node representations, the model incorporates two graph reconstruction objectives: link prediction for predicting edge labels between nodes and distance prediction between nodes in the graph.</p><!-- Meanless: 14 Behrooz Mansouri-->
<!-- Media -->
<!-- figureText: chase-01 ARG1 Graph Encoder Sequence Decoder Dog chases the cat. chase-01 ARG0 dog ARG1 cat Sequence Encoder -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_13.jpg?x=382&#x26;y=270&#x26;w=1142&#x26;h=210&#x26;r=0">
<p>Fig. 6. Neural AMR-to-text generation approaches. The top section illustrates graph-to-seq models, where a graph encoder directly processes the AMR to generate text. The bottom section shows seq-to-seq approaches, where a linearized representation of the AMR is processed by a sequence encoder.</p>
<!-- Media -->
<h3>4.3 Future: Mixed Seq/Graph-to-Seq</h3>
<p>Although current advanced large language models have not been fully investigated for AMR-to-text generation, early work such as GPT-Too [62] has explored fine-tuning a GPT-2 model on AMR data. Unlike traditional AMR-to-text approaches that often rely on specialized graph-to-seq models, GPT-Too adapts a general-purpose language model through targeted training adjustments, including cycle-consistency-based re-scoring, to better align the generated text with the underlying AMR structure, thereby improving both semantic accuracy and fluency.</p>
<p>Recent research indicates that while seq-to-seq models with AMR linearization can produce strong results, there is still significant potential in exploring different linearization strategies. For example, Hoyle et al. [41] found that models trained on fixed canonical linearizations struggled to generalize; introducing random traversal-based linearizations and scaffolding losses improved graph sensitivity, particularly in low-resource settings. Building on these findings, StructAdapt [91] proposed a novel adapter architecture that integrates graph connectivity into the encoding process. Furthermore, Montella et al. [68] explored incorporating relative positional embeddings in transformer models which enhanced generation even when the AMR structure was partially erroneous or missing, suggesting that further exploration of mixed seq/graph representations is necessary.</p>
<p>Previous research has shown that graph-based representations of AMRs generally outperform tree-based (ignoring reentrancies) and linearized representations [22]. However, each representation offers unique advantages. Recent work, such as the DualGen model [40], exemplifies this by employing a dual-encoder design, one encoder processes the AMR graph using a Graph Neural Network (GNN) to capture relationships between concepts, while the other processes a linearized sequence of AMR nodes to capture ordering information. DualGen achieves BLEU scores of 51.6 on AMR 2.0 and 51.8 on AMR 3.0, positioning it as the current state-of-the-art for AMR-to-text generation. Combining these representations effectively remains a promising future direction for text generation tasks.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 15-->
<!-- Media -->
<!-- figureText: Spanish: El niño se fue a casa. AMR Parsing go-02 ARGO ARG4 AMR-to-text boy home (B) Cross-Lingual AMR Parsing and Text Generation ir Spanish: El niño se fue a casa. German: Der Junge ging nach Hause ARGO ARG4 Chinese: 男孩回家了 Persian: La, Al's4Joung niño casa (A) Direct AMR Parsing -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_14.jpg?x=264&#x26;y=270&#x26;w=1152&#x26;h=296&#x26;r=0">
<p>Fig. 7. AMR Parsing and AMR-to-text Generation Tasks for Non-English Languages. Approaches shown are: (A) direct AMR parsing using target-language annotation guidelines; (B) cross-lingual AMR parsing to generate English AMRs from non-English sentences, and AMR-to-text generation to produce sentences in the target language from English AMRs.</p>
<!-- Media -->
<h2>5 AMR FOR NON-ENGLISH LANGUAGES</h2>
<p>Although AMR was originally developed for English, its design abstracts away from language-specific features such as word order, morphology, and function words, thereby reducing many sources of cross-linguistic variation [7]. While AMR was not intended to serve as an interlingua, this abstraction has inspired research on its utility as a cross-lingual semantic representation [113]. Early research compared manually generated AMRs for various languages to their English counterparts, identifying common semantic structures. In practice, AMR corpora for other languages are typically created in one of two approaches. One approach adapts the English annotation guidelines for the target language, with experts annotating data accordingly. In another approach, source-language sentences may be translated into English, and then existing English AMR parsers are applied to generate the AMR graphs.</p>
<p>A related task is cross-lingual AMR parsing, where a sentence in any language is converted into its corresponding English AMR graph (with nodes representing English words, PropBank framesets, or special AMR keywords). Damonte and Cohen [21] explored two strategies for this task in Italian, Spanish, German, and Chinese: annotation projection, which leverages word alignments to transfer AMR structure from one language to another, and machine translation, which first translates the input into English before applying an English AMR parser.</p>
<p>Figure 7 illustrates these common approaches. Some methods focus on language-specific annotation and parser development (e.g., training an AMR parser directly on Spanish sentences), while others concentrate on cross-lingual tasks, such as generating English AMRs from non-English text or producing target-language sentences from English AMRs.</p>
<h3>5.1 Then: Exploring Feasibility</h3>
<p>One of the first studies to explore the compatibility of non-English AMRs with English AMRs examined 100 Chinese and Czech sentences and their corresponding AMRs against English [120]. The analysis revealed that it is not always feasible to structurally align English AMRs with those from Czech and Chinese; however, refined annotation guidelines can help address some of these discrepancies. Additionally, the compatibility between English and Chinese AMRs was found to be greater than that between English and Czech AMRs. This idea was later extended by developing an AMR parser capable of handling multiple languages [105]. However, due to the lack of annotated AMR corpora and language-specific specifications, the parser would introduce errors that did not comply with the AMR format.</p>
<p>Building on these early efforts, researchers turned to leveraging literary texts to construct non-English AMR corpora. For instance, since one of the earliest AMR corpora was based on the novel The Little Prince, subsequent work utilized this novel for non-English annotation. The annotation of the Chinese translation of The Little Prince marked the first attempt at building a non-English AMR corpus [55]. \({}^{6}\) This CHAMR corpus comprises 1,562 sentences annotated following English AMR guidelines that were modified to better accommodate Chinese. In CHAMR, argument labels and predicate senses are based on the Chinese Proposition Bank, while other aspects of the annotation follow the English design. For example, if a city's name is written in Chinese, the entity is labeled as 'city' and is connected via a 'name' edge to its Chinese name.</p><!-- Meanless: 16 Behrooz Mansouri-->
<p>Later studies further explored the feasibility of AMR annotation for additional languages. In one study, 100 sentences were manually annotated for Turkish by a non-Turkish linguist, who aligned the original English sentences with their literary Turkish translations to create corresponding AMR graphs [4]. In this Turkish corpus, a few sentences share exactly the same AMR structure as their English counterparts; however, most divergences arise from differences in word choice during translation. Turkish appears to be more expressive due to its use of suffixes, which add nuances such as possession markers and intensifiers, and its frequent use of light verbs and multi-word expressions. In English AMR annotation, light verb constructions are typically removed and OntoNotes predicate frames are used to handle verb-particle combinations. Due to Turkish's highly productive morphology and idiosyncratic features, extra care is required when dealing with multi-word expressions and light verb constructions in AMR annotation.</p>
<h3>5.2 Now: Annotation Guidelines and Parsers</h3>
<p>Efforts to develop annotation guidelines for AMR have extended to several non-English languages [4, 19, 39, 66]. The primary approach involves adapting the English AMR guidelines to the target language and identifying areas where divergences occur. In many cases, proposed solutions address these discrepancies, although some issues remain open for future research. For example, Spanish AMR annotation has been studied [66], focusing on the need for language-specific adjustments, such as handling noun phrase ellipses, third-person possessives and clitic pronouns, the use of "se," gender distinctions, specific verbal constructions (e.g., verbal periphrases and locative expressions), and double negatives. Another example is the AMR 2.0 - Four Translation Corpus, which comprises Spanish, German, Italian, and Chinese Mandarin translations of a subset of sentences from AMR Annotation Release 2.0 [20].</p>
<p>Recent advancements in multilingual AMR parsing and AMR-to-text generation have moved beyond language-specific manual annotations. Modern models leverage neural architectures and multilingual pre-trained models to overcome some limitations of earlier approaches. Instead of relying solely on manual annotation guidelines or alignment strategies between English AMR and non-English corpora, these models employ techniques such as knowledge distillation and multilingual transformer architectures to enable cross-lingual parsing.</p>
<p>For instance, while Damonte and Cohen [21] utilized alignment for cross-lingual AMR parsing, XL-AMR [11] enables AMR parsing across different languages by leveraging a multilingual transformer model without requiring explicit alignment. XL-AMR incorporates two jointly trained modules for concept and relation identification. The concept identification module detects English words, AMR nodes, and PropBank framesets using a seq-to-seq architecture for improved cross-lingual performance, while the relation identification module reconstructs the AMR graph using a deep biaffine classifier. The model is trained using two strategies: (1) employing silver training data through annotation projection (where English AMR graphs are projected onto target language sentences), and (2) translating English (sentence, AMR) pairs to create gold-standard training data in the target language. The latter approach has yielded better results, consistent with later findings [103].</p>
<hr>
<!-- Footnote -->
<p>\({}^{6}\) While this corpus is also referred to as ’CAMR’,we use ’CHAMR’ to avoid confusion with CAMR parser.</p>
<!-- Footnote -->
<hr><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 17-->
<p>XAMR [16] proposed a multilingual AMR parser designed to work across all languages without requiring explicit word-to-node alignments. This approach uses an English parser as a teacher and applies knowledge distillation (both token-level and sequence-level) to train a student model that operates across multiple languages. Due to the lack of human-annotated AMR datasets for non-English languages, the authors generated silver training data by translating English sentences into other languages and applying back-translation consistency checks to ensure quality. The parser underwent multiple pre-training and fine-tuning stageswhile incorporating noisy knowledge distillation techniques.</p>
<p>Multilingual AMR-to-text generation has also been explored. In this task, the goal is to generate text in languages other than English from an AMR. Fan and Gardent [28] trained a multilingual AMR-to-text generation model for 21 different languages using EUROPARL multilingual corpus. They generated English AMRs using JAMR, which were then used as inputs for the generation task; the graph encoder and language models were pre-trained on between \({400}\mathrm{\;K}\) and \({8.2}\mathrm{M}\) (graph,text) pairs,depending on the target language. XLPT-AMR [118] represents a zero-shot multilingual AMR parser and generator pre-trained with multi-task learning that includes parsing, generation, and translation tasks. After pre-training, several fine-tuning strategies were explored. The most effective was a teacher-student-based multi-task learning approach, in which stronger English tasks served as 'teachers' to guide weaker tasks in German as 'students' Knowledge distillation was applied to align token generation probability distributions between the teacher and student, reducing noise from machine-translated German datasets.</p>
<h3>5.3 Future: Expanding AMR Corpora and Advancing Multilingual Parsers</h3>
<p>Efforts to develop AMR corpora for non-English languages are ongoing. For example, DeAMR [77] is an annotated corpus for German created by adapting the English AMR guidelines, while similar initiatives are underway for low-resource languages such as Persian [102] (PAMR), which comprises 1,020 Persian sentences and their corresponding AMRs. The study highlights challenges in AMR parsing for Persian, including issues related to translation, sense numbers, English tags and expressions, and the unique structure of Persian writing. In addition, MASSIVE-AMR [89] represents a major recent contribution, serving as the largest corpus for multilingual AMR parsing to date; it includes 84,000 text-to-graph annotations derived from 1,685 question-answer utterances across 52 languages, based on manual translations from the MASSIVE dataset [30].</p>
<p>The assessment of AMRs in other languages is also gaining attention. Wein and Schneider [114] investigate the applicability of AMRs in cross-lingual contexts, revealing that the source language significantly influences AMR structures. Translation divergences and annotator choices contribute to cross-linguistic differences, indicating that cross-lingual evaluations must account for the source language's influence to ensure accurate semantic interpretations. Despite challenges posed by lexicalization differences, AMR shows promise for applications in non-English languages, including machine translation, summarization, and event extraction, provided that these cross-linguistic nuances are carefully addressed.</p>
<p>AMR parsing and generation for non-English languages, especially low-resource ones, remain active research areas with new approaches emerging. Meta-XMAR [48] introduces meta-learning and joint-learning strategies for cross-lingual AMR parsing, focusing on underrepresented languages such as French, Chinese, Korean, Persian, and Croatian. This approach employs a seq-to-seq framework using the mBART model [101] that linearizes AMR graphs by removing variables and wiki links. By simulating k-shot learning during training, the parser adapts quickly to new languages, iteratively updating model parameters to generalize across languages.</p>
<p>Martinez and Parmentier [99] propose two efficient techniques to improve AMR-to-text generation for both high-resource and low-resource languages. They introduce Hierarchical QLoRA, a variation of curriculum learning that iteratively refines a multilingual model into monolingual models using Low-Rank Adaptation (LoRA) and 4-bit quantization for memory efficiency. Their approach fine-tunes a multilingual mT5-large model in a tree-like structure across four levels, progressively training on smaller subsets of languages to balance cross-lingual transfer and regularization. Two grouping strategies are explored: one based on maximizing the distance between languages to serve as a regularizer and the other based on phylogenetic relationships to promote transfer between closely related languages.</p><!-- Meanless: 18 Behrooz Mansouri-->
<p>Overall, future trends in non-English AMR research are likely to focus on expanding large and diverse multilingual AMR corpora, especially for low-resource languages, and on refining cross-lingual parsing and generation techniques through advanced neural architectures. Emphasis will likely shift toward leveraging multilingual pre-trained models, zero-shot learning, and meta-learning to overcome resource limitations and improve semantic consistency across languages.</p>
<h2>6 AMR APPLICATIONS</h2>
<p>AMRs are used in a wide range of natural language processing tasks. These tasks can be broadly categorized into text-to-text generation, text classification, information extraction, information seeking, and more. This section explores several applications of AMRs across these domains.</p>
<h3>6.1 Text-to-Text Generation</h3>
<p>Several natural language processing tasks involve transforming input text into output text, such as summarization and machine translation. AMR has been explored as a valuable tool for these tasks, offering a structured semantic representation of text.</p>
<p>Summarization. The application of AMRs for summarization began with abstractive summarization, where the goal is to generate summaries by rephrasing the main ideas of the input text into new sentences. Liu et al. [59] introduced an approach in which each sentence is first parsed into an AMR graph. These graphs are then merged and transformed into a single summary AMR graph that is passed to a text generator to produce the final summary. An example of this process is shown in Fig. 8, which summarizes two sentences: "I saw Joe's dog, which was running in the garden" and "The dog was chasing a cat." In the merging step, graph fragments are collapsed into unified concepts by merging nodes with the same label across different sentences. The sub-graph prediction is formulated as an integer linear programming optimization problem to select the most important parts of the merged AMR graph for final summary generation. A similar approach is used for multi-document summarization [58], where the summary is generated from the PENMAN representation of the summary AMR.</p>
<p>Despite the progress made, early work on summarization faced challenges in generating fluent language from AMRs, often producing outputs that resembled a bag-of-words collection. To address these issues, the AMR2Text model [37] introduced a guided language generation approach. Using a seq-to-seq model with attention, the guided model combined decoder probability distributions with those derived from the summary AMR. Sentences exhibiting the highest similarity to the summary AMR graph were identified using the Longest Common Subsequence (LCS) method, and n-gram probabilities were interpolated to refine word selection. Experimental results demonstrated that this guided approach produced more coherent summaries compared to unguided baselines.</p>
<p>AMRs have also been explored for extractive summarization, which aims to retain all critical information by directly selecting salient portions of the text while avoiding incoherencies or unresolved anaphoric references. Mishra and Gayen [67] proposed a method that applies pairwise coreference resolution followed by AMR generation for individual sentences. The resulting AMRs are merged into a comprehensive graph representing the entire text, from which key summary segments are then extracted.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 19-->
<!-- Media -->
<!-- figureText: see-01 chase-01 ARG1 ARGO chase-01 ARG1 Introcation cat dog graden cat poss person name name op1 “Joe” ARG0 ARG1 ARG0 dog dog ARG0-of person run-02 name location name garden op1 AMR, \( {\mathrm{{AMR}}}_{2} \) -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_18.jpg?x=204&#x26;y=268&#x26;w=1284&#x26;h=520&#x26;r=0">
<p>Fig. 8. Application of AMR for summarization (Adapted from Liu et al [59]). \({AM}{R}_{1}\) shows the AMR for the sentence "I saw Joe's dog, which was running in the garden." and \({AM}{R}_{2}\) shows the AMR for the sentence "The dog was chasing a cat." The summary graph is represented as \({AM}{R}_{Summary}\) .</p>
<!-- Media -->
<p>AMRs are particularly useful for handling complex entity interactions, such as in TV series transcripts. AMRTVSumm [42] leverages AMRs to represent individual scenes within an episode. These scene-level AMRs are subsequently integrated into a hierarchical encoder-decoder model that incorporates a novel cross-level cross-attention mechanism. This approach builds on dialogue-specific AMRs introduced by Bai et al. [5], which enhance traditional AMRs by including speaker nodes, utterance nodes, and improved coreference handling to better capture conversational dynamics.</p>
<p>Machine Translation. Defined as the task of converting source text into a target language, machine translation has also benefited from AMR-based approaches that integrate semantic information from AMR graphs into translation models. Early work by Song et al. [96] used a graph recurrent network to encode AMR graphs, alongside a Bi-LSTM to encode the source text. The decoder (a doubly attentive LSTM) generated translations by attending to both graph-based and sequential encodings. This dual-encoding strategy effectively leveraged the semantic structure provided by AMRs, laying the groundwork for future innovations. A similar approach was later proposed by modifying the model architecture and exploring graph encoders with attention for encoding AMRs [72]. In this study, various neural machine translation models were examined, including seq-to-seq models with Bi-LSTM and transformers, with the latter reported as less effective than the former in this context.</p>
<p>AMR-NMT [56] integrates AMRs into the transformer architecture for neural machine translation. The proposed model combines a transformer with a heterogeneous graph transformer, enabling it to encode both source sentences and their corresponding AMR graphs. The architecture (AMR-Transformer) features parallel stacked encoder and decoder layers: one set processes the sequence data, while the other handles the graph data. The two resulting representations are then integrated to generate the final translation. This approach leverages the strengths of both sequence encoding and graph-based semantic encoding, resulting in improved translation quality. Experimental results indicate that the AMR-Transformer outperforms both the standard transformer model and previous non-transformer-based models across two different language pairs in high-resource and low-resource settings.</p><!-- Meanless: 20 Behrooz Mansouri-->
<!-- Media -->
<!-- figureText: Graph/Text Embedding AMR Features Combined Classifier Prediction Features Textual Features AMR Parser or Feature Selection AMR Input Text Text Embedding or Feature Selection -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_19.jpg?x=318&#x26;y=274&#x26;w=1272&#x26;h=236&#x26;r=0">
<p>Fig. 9. Application of AMR for text classification. The input text is parsed to generate the AMR. Features from AMR can be derived from linearization and text embeddings, graph embeddings, or other designed features. The textual features are extracted from raw text and then combined with AMR features. These features are passed to a classifier to predict a label.</p>
<!-- Media -->
<p>A novel application of AMRs in machine translation was explored by Wein et al. [115] in the context of translationese reduction. The term "translationese" refers to linguistic features, such as semantic patterns and unique syntactic structures, that are characteristic of human-translated texts. Reducing translationese can enhance the evaluation of machine translation models. The proposed parse-then-generate technique first parses a text affected by translationese into an AMR and then generates text from that AMR. Although the analysis revealed that AMR generation struggles to produce fluent texts, it remains the only method (compared to T5-based and BART-based baselines) that contributes to translationese reduction, thereby confirming the promise of AMRs as an Interlingua.</p>
<h3>6.2 Text Classification</h3>
<p>Text classification involves assigning categories to textual data, supporting tasks such as sentiment analysis, paraphrase detection, and fake news detection. The proposed models with AMRs often combine the AMR features with textual features for classification, as shown in Fig. 9. For instance, FakEDAMR [36] encodes textual content using AMRs for fake news detection. Using GloVe embeddings for text, and graph embeddings of AMRs, these embeddings are concatenated and passed to a BiLSTM classification layer. Another example is AMR-CNN [26] which integrates AMR with a Convolutional Neural Network (CNN) to detect the level of profanity and toxicity in online text content. Unlike traditional toxic content detection systems that rely on lexicon- and keyword-based methods, AMR-CNN focuses on understanding the underlying meaning of sentences. AMR allows for the extraction of semantic roles, entity types, coreference resolution, and other nuanced aspects of language, such as modality and polarity, making it well-suited for the task of detecting abusive or toxic content.</p>
<p>For sentiment analysis, the AMR-based Path Aggregation Relational Network (APARN) [61] model investigates replacing dependency trees with AMRs. In this model, the input text is first parsed into AMR with the SPRING model [10]. Then, the AMR is aligned using LEAMR [12] to rebuild AMR relations between words in the sentence and AMR nodes. Using BERT, embeddings for words in sentences, as well as AMR nodes and edges are generated. These embeddings are passed to the path aggregator component that combines them into a relational feature matrix. Finally, the output weight matrix from the path aggregator is added to the self-attention matrix from the original sentence and passed to a fully connected softmax layer to classify sentiment. The proposed approach achieved a higher accuracy compared to using BERT alone,with improvements of 2-5% across different datasets).</p>
<p>AMRs also serve as a suitable representation for Natural Language Inference (NLI), the task of determining whether a given premise entails, contradicts, or is neutral with respect to a hypothesis. AMR4NLI [76] studied whether AMRs could represent the premises and hypotheses more effectively (through WWLK metric), allowing a more interpretable way to measure if the hypothesis is a semantic substructure of the premise. The results show that while AMRs capture the semantic structure and relationships explicitly, contextualized embeddings (through BERTScore) provide a rich, dense representation of meaning. By combining these two through a linear combination of graph and text similarity metrics, they created a hybrid model that leveraged the strengths of both approaches. This hybrid model was tested across several English NLI benchmarks, showing improved robustness.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 21-->
<p>Another task where AMRs are beneficial is paraphrase detection, deciding whether two sentences are paraphrases of each other. Issa et al. [44] explored AMRs for this task. While the naïve application of AMR parsing (as a bag of words with SMatch) did not yield satisfactory results, the authors combined AMR with Latent Semantic Analysis (LSA) to better capture the semantic similarities between sentences. LSA creates a sentence-term matrix based on TF-IDF, and AMR is used to re-weight the values in this matrix. By applying PageRank to the AMR graphs, the most influential nodes were identified in each graph. This helped in focusing on the most critical parts of the sentence when determining whether two sentences were paraphrases.</p>
<p>To build paraphrase dataset, the PARAAMR [43] leverages AMR-to-text generation to generate syntactically diverse paraphrases. Traditional methods of generating paraphrases, such as machine back-translation, often lack syntactic variety, generating sentences that are too similar structurally. PARAAMR addresses this by using AMR to create paraphrases with greater syntactic diversity while maintaining semantic consistency. In this approach, the AMR for input text is modified by changing the focus. To change the focus, nodes are randomly selected as the new focus, and the incoming edges for that node are reversed. The resulting AMR is then passed to an AMR-to-text generation pipeline to produce the paraphrase. Another approach to generate paraphrases with AMR is SAPG ("Semantically-Aware Paraphrase Generation with AMR Graphs") [100] that encodes AMR graphs of input texts using a graph neural network-based encoder integrated with a pretrained language model. SAPG employs a dual-encoder architecture. One encoder processes a linearized version of the AMR graph, while the other uses a Graph Neural Network (GNN) to encode the AMR graph structure directly into a pretrained language model.</p>
<p>A recent similar application to paraphrase detection was used in DART model [82] to detect AI-generated text. The core idea is that rephrasing a text can reveal semantic differences between human and AI writing styles. DART leverages AMR as a semantic representation to capture the underlying meaning of both the original text and its rephrased versions. The framework parses texts into AMR graphs and then uses SEMA (recall and precision) between the AMR of the original text and the AMRs of its rephrased counterparts. These similarity scores are then passed to a classifier to detect AI-generated text. The experimental results shows this model achieve the highest F1-score compared to state-of-the-art models.</p>
<h3>6.3 Information Extraction</h3>
<p>Information Extraction (IE) aims to extract structured information from unstructured text, but traditional methods often struggle to capture deep semantics. AMR enhances IE by providing semantic graphs that capture concepts, relationships, and roles, enabling more accurate event and argument extraction across various domains. For example, Li et al. [57] explored event detection with AMR to identify and classify events. The AMR graph is analyzed to identify event triggers, which are words or phrases that indicate the occurrence of an event. To this end, a Maximum Entropy classifier is used for event classification using textual features (e.g., lexical features), along with AMR features. The AMR features are defined around the trigger node (which indicates the occurrence of an event) and include features such as the distance to the root, the node value, the parent node, sibling nodes, and children. The analysis shows that classification accuracy can improve by \(2\%\) when AMR features are incorporated.</p><!-- Meanless: 22 Behrooz Mansouri-->
<p>The TSAR (Two-Stream Abstract meaning Representation) [119] model presents a novel approach to improve the extraction of event arguments from documents. Event argument extraction focuses on identifying the entities that serve as event arguments and predicting their roles in events. The two-stream encoding module in this work employs both global and local encoders to capture different levels of context. First, with the same BERT-based pre-trained model, global and local embeddings are generated from the input text. Two types of AMRs are then generated; one as a local AMR based on sentences, and the other as a global AMR by connecting the roots of local AMRs. These AMRs are used by the AMR-guided interaction module to generate local and global embeddings, which are then fused and passed to a classifier.</p>
<p>AMRs are also used for the identification of biomedical events, defined as the automatic identification and classification of specific biological events, such as gene expressions and regulatory relationships [50]. Rao et al. viewed this as a subgraph identification problem [88]. They developed a neural network model to identify these event subgraphs and employed a distant supervision technique to generate additional training data. Their approach was evaluated using the 2013 Genia Event Extraction dataset, yielding promising results for the application of AMR in biomedical text analysis. The Knowledge-AMR [122] is another model that enriches AMRs with an external knowledge base to make them more effective for extracting fine-grained biomedical information. This involves incorporating biomedical knowledge into AMR to improve its expressiveness and accuracy in representing the semantics of biomedical texts. The results show that the enriched AMR can be effective for extracting complex biomedical entities and relations, including fine-grained relations between biological processes, molecular functions, and disease mechanisms from literature, which is typically more challenging in biomedical texts.</p>
<p>Another similar application is drug-drug interactions (DDI). DDI studies evaluate how the presence of one drug alters the effects of another. Wang et al. [112] viewed DDI as a binary classification task, with drug-drug pairs are classified as either positive (indicating interaction) or negative. Since many drug names may not appear in the AMR bank, first the word "drug" is replaced with "medication", and then all drug entities are substituted with the word "drug". DDI sentences are then parsed with JAMR into AMR graphs, followed by restoring the drug entities. After that, the AMR is passed to the SkipGram model to generate AMR embeddings. Along with AMR embeddings, dependency parse graphs, and raw text embedding features were used for classification. The results indicate that while AMRs can provide useful information, they should be used with raw text embeddings for improvement to yield better effectiveness.</p>
<h3>6.4 Information Seeking</h3>
<p>AMR is effective for information retrieval and question answering because it captures the underlying semantic structure of sentences, enabling a deeper understanding of meaning. By representing concepts, relationships, and events in a graph-like structure, AMR allows for more accurate matching of user queries with relevant information, especially in complex domains like biomedicine and mathematics. For example, InfoForager [14] uses AMR to find answers related to COVID-19. Sentences in the collection are first parsed into their AMR graphs, which are then matched against the input question AMR using SMatch for ranking retrieved sentences.</p>
<p>MathAMR [65] integrates mathematical formula representations with AMRs for the contextualized formula search, forming a unified search representation (as shown in Fig. 2). The goal of this task is to find relevant formulae to a formula query within the same context. Math formulas are usually represented in the form of operator trees, which show what operations are applied to what operands. This representation is very similar to AMRs for raw text. Therefore, in this approach, first formulas are masked with placeholders and their context is parsed to an AMR. The operator tree is then reconnected to the AMR by linking the root of the operator tree to the placeholder node for the formula. For ranking, the AMRs of the query and candidates are linearized with the depth-first search (ignoring edge labels), and the Sentence-BERT [90] model is used to calculate the similarity.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 23-->
<!-- Media -->
<!-- figureText: direct-01 :ARG0 direct-01 :ARG1 person movie :name :name name name :op1 :op2 Francis Ford Coppola The Godfathe :ARG0 (ARG1 amr-unknown movie :name name :op1 :op2 The Godfather -->
<img src="https://cdn.noedgeai.com/bo_d16gie77aajc7388kcp0_22.jpg?x=206&#x26;y=268&#x26;w=1275&#x26;h=361&#x26;r=0">
<p>Fig. 10. Application of AMR for Question-Answering. The AMR on the left corresponds to the question 'Who directed The Godfather movie?' and the AMR on the right represents the answer 'The Godfather is directed by Francis Ford Coppola'</p>
<!-- Media -->
<p>AMRs have also been explored for semantic information retrieval in the domain of remote sensing and image exploitation [69]. The authors adapted and evaluated an AMR-based search approach to find relevant documents and extract pertinent content based on natural language queries. SMatch and SemBleu were studied as AMR similarity metrics to determine their suitability for this task. The study found that while both metrics yield promising results in ranking sentences, SemBleu is more viable for practical applications due to its lower sensitivity to grammatical variations and its ability to handle synonyms effectively.</p>
<p>AMRs are further explored for question answering. A special node, 'amr-unknown' is used in place of the answer to the question, making this representation particularly suitable for this tasks. For instance, for the question 'Who directed The Godfather movie?' and its corresponding answer 'The Godfather is directed by Francis Ford Coppola' the AMRs are shown in Fig. 10. As shown, the 'amr-unknown' node in the question AMR can be matched with the left subgraph from the answer AMR, while the root and the right subgraphs remain the same.</p>
<p>Building on these capabilities, Neuro-Symbolic Question Answering (NSQA) [49] uses AMRs to improve question understanding and logical query formation in Knowledge Base Question Answering (KBQA). KBQA is the task of answering natural language questions based on facts in a knowledge base. NSQA generates AMR for a given question, and then uses a novel graph transformation technique to generate candidate logical queries aligned with a knowledge base. These logical queries are then Logical Neural Network to produce the final answer. Another example is the Question Decomposition method based on AMR (QDAMR) [24], introduced for multi-hop question answering. QDAMR utilizes AMR to achieve interpretable reasoning by decomposing complex, multi-hop questions into simpler sub-questions. This is achieved by segmenting the AMR graph based on reasoning types. The resulting sub-questions are answered sequentially using an existing QA model. This approach not only improves multi-hop QA performance but also generates well-formed sub-questions that outperform those produced by other question decomposition methods. By integrating interpretability directly into the QA process, QDAMR provides a clearer reasoning path. However, future research is required to assess the utility of its explanations for human users.</p>
<h3>6.5 Other Applications</h3>
<p>While we have explored some of the common downstream applications of AMRs, their utility extends to many other areas. These include tasks such as fact verification [45, 87], legal judgement prediction [107], and time-line generation [64]. AMRs have also been applied to data augmentation. AMR-DA [93] leverages AMRs to enhance training datasets by converting sentences into AMR graphs, applying augmentation strategies to modify these graphs, and generating new sentence variations from the altered graphs. AMR-DA integrates sentence-level techniques like back translation with token-level approaches such as Easy Data Augmentation (EDA) to create a more robust data augmentation framework. The method is evaluated on two tasks: semantic textual similarity (STS) and text classification. Results show that AMR-DA enhances state-of-the-art models' performance on multiple STS benchmarks and outperforms existing augmentation techniques like EDA in text classification tasks. This work is the first to utilize AMR for data augmentation, showing that it can effectively diversify training data and improve model robustness without requiring decoder retraining.</p><!-- Meanless: 24 Behrooz Mansouri-->
<p>Another generative data augmentation methodology, ABEX [33], focuses on enhancing low-resource Natural Language Understanding (NLU) tasks. ABEX employs a two-step process: first, it converts documents into AMR graphs to capture their semantic structure, and then it edits these graphs to generate abstract descriptions by removing non-essential details while retaining the core meaning. This approach is training-free and flexible, enabling it to adapt to various tasks without the need for extensive model retraining. ABEX also addresses data scarcity by synthesizing large-scale datasets of abstract-document pairs, using prompts to large language models (LLMs) to create abstract descriptions. Its innovative use of AMR editing and generative augmentation enhances data diversity and quality, providing a robust solution for low-resource scenarios.</p>
<p>AMRs are also used for image-text processing, particularly for tasks like dense image captioning. Dense captioning involves generating multiple textual descriptions for various regions of an image. For instance, an image of a girl riding a bicycle could yield descriptions such as "a girl riding a bicycle," the girl who rides a bicycle," or "the girl bicycle rider." To address the variability in descriptions, Antonio et al. [71] utilizes DenseCap [47], comparing training the model with natural language and different AMR variants: (1) linearized AMR, (2) anonymized linearized AMR (replacing named entities and quantifiers with placeholders), and (3) concatenated anonymized AMR (merging concepts and arguments). For evaluation, natural language targets were converted to AMRs using the same pipeline. Results showed that the concatenated anonymized AMR input was most effective, achieving nearly double the mean average precision (mAP) compared to natural language. Future work could investigate using AMR-to-text models for a fair comparison with natural text.</p>
<p>For visual scene understanding, Meta-AMR graphs [1] address the limitations of traditional scene graphs (SGs). Scene graphs abstract visual inputs into structured representations, focusing on spatial relationships between entities, but they require extensive manual annotation and lack higher-level semantic information. In contrast, AMR graphs capture abstract semantic concepts, offering a richer understanding of scenes. The proposed approach parses images into AMRs by leveraging text-to-AMR parsers and introduces meta-AMR graphs to unify information from multiple image descriptions. The methodology involves two models: one that directly generates AMRs from images and another that operates in two stages, predicting nodes first and then their relationships. Both models use visual features extracted from images and are trained on datasets pairing images with captions and corresponding AMR graphs, enhancing the representation and understanding of visual scenes.</p>
<h2>7 CONCLUSION: THEN, NOW, FUTURE</h2>
<p>This survey explored Abstract Meaning Representation (AMR), its intended use, and its applications in natural language processing tasks. AMR serves as a semantic representation that captures "who is doing what to whom." AMR parsers generate these representations for a given text, with current state-of-the-art models predominantly based on seq-to-se architectures. The application of large language models (LLMs) for AMR parsing is an emerging area of research. While there have been efforts to develop parsers capable of handling longer text spans, most AMR parsers remain focused on sentence-level parsing. Furthermore, although AMR was not initially designed for non-English languages, several approaches have been proposed to extend its capabilities to multilingual parsing.</p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 25-->
<p>AMRs have continued to evolve over time, with various extensions and enrichment strategies being introduced. One notable extension is Uniform Meaning Representation (UMR) [104], which aims to support cross-linguistic plausibility and portability. UMRs are currently a focus of active research, and it is possible that they could eventually replace traditional AMRs in some applications.</p>
<p>Text generation from AMRs is another domain of active research discussed in this survey. This task focuses on generating corresponding text from an AMR while preserving the semantic integrity of the original input. Similar to AMR parsing, current approaches for text generation are largely based on sequence-to-sequence models, with LLMs being increasingly explored for this purpose. As AMRs become more integrated into text generation workflows, they may eventually be incorporated into LLMs to improve the semantic quality of generated text.</p>
<p>Finally, we explored various downstream applications of AMRs, including text-to-text generation, text classification, information extraction, information seeking, and tasks like caption generation. Initially designed to abstract meaning, AMRs were first applied to related tasks such as summarization. However, they are now gaining traction in multimodal tasks, where AMR features are combined with data types such as images. Another emerging application is data augmentation, where pipelines generate text from AMRs to enrich training datasets.</p>
<p>Looking ahead, AMRs are likely to find novel applications in tasks such as fairness, visual question-answering, and story generation. By abstracting and representing meaning effectively, AMRs have the potential to provide valuable features that enhance system performance and improve task outcomes. REFERENCES</p>
<p>[1] Mohamed Ashraf Abdelsalam, Zhan Shi, Federico Fancellu, Kalliopi Basioti, Dhaivat Bhatt, Vladimir Pavlovic, and Afsaneh Fazly. 2022. Visual Semantic Parsing: From Images to Abstract Meaning Representation. In Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL), Antske Fokkens and Vivek Srikumar (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid), 282-300. <a href="https://doi.org/10.18653/v1/2022.conll-1.19">https://doi.org/10.18653/v1/2022.conll-1.19</a></p>
<p>[2] Omri Abend and Ari Rappoport. 2017. The State of the Art in Semantic Representation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 77-89. <a href="https://doi.org/10.18653/v1/P17-1008">https://doi.org/10.18653/v1/P17-1008</a></p>
<p>[3] Rafael T. Anchieta, Marco A. S. Cabezudo, and Thiago A. S. Pardo. 2019. SEMA: an Extended Semantic Evaluation Metric for AMR. arXiv:1905.12069 [cs.CL] <a href="https://arxiv.org/abs/1905.12069">https://arxiv.org/abs/1905.12069</a></p>
<p>[4] Zahra Azin and Gülsen Eryigit. 2019. Towards Turkish Abstract Meaning Representation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, Fernando Alva-Manchego, Eunsol Choi, and Daniel Khashabi (Eds.). Association for Computational Linguistics, Florence, Italy, 43-47. <a href="https://doi.org/10.18653/v1/P19-2006">https://doi.org/10.18653/v1/P19-2006</a></p>
<p>[5] Xuefeng Bai, Yulong Chen, Linfeng Song, and Yue Zhang. 2021. Semantic Representation for Dialogue Modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 4430-4445. <a href="https://doi.org/10.18653/v1/2021.acl-long.342">https://doi.org/10.18653/v1/2021.acl-long.342</a></p>
<p>[6] Xuefeng Bai, Yulong Chen, and Yue Zhang. 2022. Graph Pre-training for AMR Parsing and Generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 6001-6015. <a href="https://doi.org/10.18653/v1/2022.acl-long.415">https://doi.org/10.18653/v1/2022.acl-long.415</a></p>
<p>[7] Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, Antonio Pareja-Lora, Maria Liakata, and Stefanie Dipper (Eds.). Association for Computational Linguistics, Sofia, Bulgaria, 178-186. <a href="https://aclanthology.org/W13-2322">https://aclanthology.org/W13-2322</a></p><!-- Meanless: 26 Behrooz Mansouri-->
<p>[8] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss (Eds.). Association for Computational Linguistics, Ann Arbor, Michigan, 65-72. <a href="https://aclanthology.org/W05-">https://aclanthology.org/W05-</a> 0909</p>
<p>[9] Michele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 12564-12573.</p>
<p>[10] Michele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline. Proceedings of the AAAI Conference on Artificial Intelligence 35, 14 (May 2021), 12564-12573. https: //doi.org/10.1609/aaai.v35i14.17489</p>
<p>[11] Rexhina Blloshmi, Rocco Tripodi, and Roberto Navigli. 2020. XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 2487-2500. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.195">https://doi.org/10.18653/v1/2020.emnlp-main.195</a></p>
<p>[12] Austin Blodgett and Nathan Schneider. 2021. Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 3310-3321. <a href="https://doi.org/10.18653/v1/2021.acl-long.257">https://doi.org/10.18653/v1/2021.acl-long.257</a></p>
<p>[13] Claire Bonial, Lucia Donatelli, Mitchell Abrams, Stephanie M. Lukin, Stephen Tratz, Matthew Marge, Ron Artstein, David Traum, and Clare Voss. 2020. Dialogue-AMR: Abstract Meaning Representation for Dialogue. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 684-695. <a href="https://aclanthology.org/2020.lrec-1.86">https://aclanthology.org/2020.lrec-1.86</a></p>
<p>[14] Claire Bonial, Stephanie M. Lukin, David Doughty, Steven Hill, and Clare Voss. 2020. InfoForager: Leveraging Semantic Search with AMR for COVID-19 Research. In Proceedings of the Second International Workshop on Designing Meaning Representations, Nianwen Xue, Johan Bos, William Croft, Jan Hajič, Chu-Ren Huang, Stephan Oepen, Martha Palmer, and James Pustejovsky (Eds.). Association for Computational Linguistics, Barcelona Spain (online), 67-77. <a href="https://aclanthology.org/2020.dmr-1.7">https://aclanthology.org/2020.dmr-1.7</a></p>
<p>[15] Richard Brutti, Lucia Donatelli, Kenneth Lai, and James Pustejovsky. 2022. Abstract Meaning Representation for Gesture. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christophe Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 1576-1583. <a href="https://aclanthology.org/2022.lrec-1.169">https://aclanthology.org/2022.lrec-1.169</a></p>
<p>[16] Deng Cai, Xin Li, Jackie Chun-Sing Ho, Lidong Bing, and Wai Lam. 2021. Multilingual AMR Parsing with Noisy Knowledge Distillation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 2778-2789. <a href="https://doi.org/10.18653/v1/2021.findings-emnlp.237">https://doi.org/10.18653/v1/2021.findings-emnlp.237</a></p>
<p>[17] Shu Cai and Kevin Knight. 2013. Smatch: an Evaluation Metric for Semantic Feature Structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Hinrich Schuetze, Pascale Fung, and Massimo Poesio (Eds.). Association for Computational Linguistics, Sofia, Bulgaria, 748-752. <a href="https://aclanthology.org/P13-2131">https://aclanthology.org/P13-2131</a></p>
<p>[18] Ziming Cheng, Zuchao Li, and Hai Zhao. 2022. BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning. In Proceedings of the 29th International Conference on Computational Linguistics, Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.). International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 5461-5475. <a href="https://aclanthology.org/2022.coling-1.485">https://aclanthology.org/2022.coling-1.485</a></p>
<p>[19] Hyonsu Choe, Jiyoon Han, Hyejin Park, Tae Hwan Oh, and Hansaem Kim. 2020. Building Korean Abstract Meaning Representation Corpus. In Proceedings of the Second International Workshop on Designing Meaning Representations, Nianwen Xue, Johan Bos, William Croft, Jan Hajič, Chu-Ren Huang, Stephan Oepen, Martha Palmer, and James Pustejovsky (Eds.). Association for Computational Linguistics, Barcelona Spain (online), 21-29. <a href="https://aclanthology.org/2020.dmr-1.3">https://aclanthology.org/2020.dmr-1.3</a></p>
<p>[20] Marco Damonte and Shay Cohen. 2022. Abstract Meaning Representation 2.0 - Four Translations. <a href="https://doi.org/11272.1/AB2/50U0AQ">https://doi.org/11272.1/AB2/50U0AQ</a></p>
<p>[21] Marco Damonte and Shay B. Cohen. 2018. Cross-Lingual Abstract Meaning Representation Parsing. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 1146-1155. <a href="https://doi.org/10.18653/v1/N18-1104">https://doi.org/10.18653/v1/N18-1104</a></p>
<p>[22] Marco Damonte and Shay B. Cohen. 2019. Structural Neural Encoders for AMR-to-text Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 3649-3658. https: //doi.org/10.18653/v1/N19-1366</p>
<p>[23] Marco Damonte, Shay B. Cohen, and Giorgio Satta. 2017. An Incremental Parser for Abstract Meaning Representation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Mirella Lapata, Phil Blunsom, and Alexander Koller (Eds.). Association for Computational Linguistics, Valencia, Spain, 536-546. <a href="https://aclanthology.org/E17-1051">https://aclanthology.org/E17-1051</a></p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 27-->
<p>[24] Zhenyun Deng, Yonghua Zhu, Yang Chen, Michael Witbrock, and Patricia Riddle. 2022. Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4093-4099. <a href="https://doi.org/10.24963/ijcai.2022/568">https://doi.org/10.24963/ijcai.2022/568</a> Main Track.</p>
<p>[25] Lucia Donatelli, Michael Regan, William Croft, and Nathan Schneider. 2018. Annotation of Tense and Aspect Semantics for Sentential AMR. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Agata Savary, Carlos Ramisch, Jena D. Hwang, Nathan Schneider, Melanie Andresen, Sameer Pradhan, and Miriam R. L. Petruck (Eds.). Association for Computational Linguistics, Santa Fe, New Mexico, USA, 96-108. <a href="https://aclanthology.org/W18-4912">https://aclanthology.org/W18-4912</a></p>
<p>[26] Ermal Elbasani and Jeong-Dong Kim. 2022. AMR-CNN: Abstract Meaning Representation with Convolution Neural Network for Toxic Content Detection. Journal of Web Engineering 21, 3 (2022), 677-692. <a href="https://doi.org/10.13052/jwe1540-9589.2135">https://doi.org/10.13052/jwe1540-9589.2135</a></p>
<p>[27] Allyson Ettinger, Jena Hwang, Valentina Pyatkin, Chandra Bhagavatula, and Yejin Choi. 2023. "You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 8250-8263. <a href="https://doi.org/10.18653/v1/2023.findings-emnlp.553">https://doi.org/10.18653/v1/2023.findings-emnlp.553</a></p>
<p>[28] Angela Fan and Claire Gardent. 2020. Multilingual AMR-to-Text Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 2889-2901. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.231">https://doi.org/10.18653/v1/2020.emnlp-main.231</a></p>
<p>[29] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT Sentence Embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 878-891. <a href="https://doi.org/10.18653/v1/2022.acl-long.62">https://doi.org/10.18653/v1/2022.acl-long.62</a></p>
<p>[30] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. 2023. MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 4277-4302. <a href="https://doi.org/10.18653/v1/2023.acl-long.235">https://doi.org/10.18653/v1/2023.acl-long.235</a></p>
<p>[31] Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from Abstract Meaning Representation using Tree Transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Kevin Knight, Ani Nenkova, and Owen Rambow (Eds.). Association for Computational Linguistics, San Diego, California, 731-739. https: //doi.org/10.18653/v1/N16-1087</p>
<p>[32] Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A Discriminative Graph-Based Parser for the Abstract Meaning Representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Kristina Toutanova and Hua Wu (Eds.). Association for Computational Linguistics, Baltimore, Maryland, 1426-1436. <a href="https://doi.org/10.3115/v1/P14-1134">https://doi.org/10.3115/v1/P14-1134</a></p>
<p>[33] Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Chandra Kiran Evuru, Ramaneswaran S, S Sakshi, and Dinesh Manocha. 2024. ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 726-748. <a href="https://doi.org/10.18653/v1/2024.acl-long.43">https://doi.org/10.18653/v1/2024.acl-long.43</a></p>
<p>[34] Daniel Gildea and Daniel Jurafsky. 2000. Automatic Labeling of Semantic Roles. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (Hong Kong) (ACL ’00). Association for Computational Linguistics, USA, 512-520. <a href="https://doi.org/10.3115/1075218.1075283">https://doi.org/10.3115/1075218.1075283</a></p>
<p>[35] Jonas Groschwitz, Shay Cohen, Lucia Donatelli, and Meaghan Fowlie. 2023. AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10728-10752. <a href="https://doi.org/10.18653/v1/2023.emnlp-main.662">https://doi.org/10.18653/v1/2023.emnlp-main.662</a></p>
<p>[36] Shubham Gupta, Narendra Yadav, Suman Kundu, and Sainathreddy Sankepally. 2024. FakEDAMR: Fake News Detection Using Abstract Meaning Representation Network. In Complex Networks &#x26; Their Applications XII, Hocine Cherifi, Luis M. Rocha, Chantal Cherifi, and Murat Donduran (Eds.). Springer Nature Switzerland, Cham, 308-319.</p>
<p>[37] Hardy Hardy and Andreas Vlachos. 2018. Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 768-773. <a href="https://doi.org/10.18653/v1/D18-1086">https://doi.org/10.18653/v1/D18-1086</a></p>
<p>[38] Johannes Heinecke. 2023. metAMoRphosED, a graphical editor for Abstract Meaning Representation. In Proceedings of the 19th Joint ACL-ISO Workshop on Interoperable Semantics (ISA-19), Harry Bunt (Ed.). Association for Computational Linguistics, Nancy, France, 27-32. https: //aclanthology.org/2023.isa-1.4/</p>
<p>[39] Johannes Heinecke and Anastasia Shimorina. 2022. Multilingual Abstract Meaning Representation for Celtic Languages. In Proceedings of the 4th Celtic Language Technology Workshop within LREC2022, Theodorus Fransen, William Lamb, and Delyth Prys (Eds.). European Language Resources Association, Marseille, France, 1-6. <a href="https://aclanthology.org/2022.cltw-1.1">https://aclanthology.org/2022.cltw-1.1</a></p>
<p>[40] Yining Hong, Fanchao Qi, and Maosong Sun. 2024. Two Heads Are Better Than One: Exploiting Both Sequence and Graph Models in AMR-To-Text Generation. (2024).</p>
<p>[41] Alexander Miserlis Hoyle, Ana Marasović, and Noah A. Smith. 2021. Promoting Graph Awareness in Linearized Graph-to-Text Generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 944-956. <a href="https://doi.org/10.18653/v1/2021.findings-acl.82">https://doi.org/10.18653/v1/2021.findings-acl.82</a></p><!-- Meanless: 28 Behrooz Mansouri-->
<p>[42] Yilun Hua, Zhaoyuan Deng, and Zhijie Xu. 2022. AMRTVSumm: AMR-augmented Hierarchical Network for TV Transcript Summarization. In Proceedings of The Workshop on Automatic Summarization for Creative Writing, Kathleen Mckeown (Ed.). Association for Computational Linguistics, Gyeongju, Republic of Korea, 36-43. <a href="https://aclanthology.org/2022.creativesumm-1.6">https://aclanthology.org/2022.creativesumm-1.6</a></p>
<p>[43] Kuan-Hao Huang, Varun Iyer, I-Hung Hsu, Anoop Kumar, Kai-Wei Chang, and Aram Galstyan. 2023. ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 8047-8061. <a href="https://doi.org/10.18653/v1/2023.acl-long.447">https://doi.org/10.18653/v1/2023.acl-long.447</a></p>
<p>[44] Fuad Issa, Marco Damonte, Shay B. Cohen, Xiaohui Yan, and Yi Chang. 2018. Abstract Meaning Representation for Paraphrase Detection. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 442-452. <a href="https://doi.org/10.18653/v1/N18-1041">https://doi.org/10.18653/v1/N18-1041</a></p>
<p>[45] Chathuri Jayaweera, Sangpil Youm, and Bonnie J Dorr. 2024. AMREx: AMR for Explainable Fact Verification. In Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER), Michael Schlichtkrull, Yulong Chen, Chenxi Whitehouse, Zhenyun Deng, Mubashara Akhtar, Rami Aly, Zhijiang Guo, Christos Christodoulopoulos, Oana Cocarascu, Arpit Mittal, James Thorne, and Andreas Vlachos (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 234-244. <a href="https://aclanthology.org/2024.fever-1.26">https://aclanthology.org/2024.fever-1.26</a></p>
<p>[46] Yuxin Ji, Gregor Williamson, and Jinho D. Choi. 2022. Automatic Enrichment of Abstract Meaning Representations. In Proceedings of the 16th Linguistic Annotation Workshop (LAW-XVI) within LREC2022, Sameer Pradhan and Sandra Kuebler (Eds.). European Language Resources Association, Marseille, France, 160-169. <a href="https://aclanthology.org/2022.law-1.19">https://aclanthology.org/2022.law-1.19</a></p>
<p>[47] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. DenseCap: Fully Convolutional Localization Networks for Dense Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>[48] Jeongwoo Kang, Maximin Coavoux, Cédric Lopez, and Didier Schwab. 2024. Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 43-51. <a href="https://doi.org/10.18653/v1/">https://doi.org/10.18653/v1/</a> 2024.findings-emnlp. 3</p>
<p>[49] Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander Gray, Ramón Fernandez Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, Francois Luus, Ndivhuwo Makondo, Nandana Mihindukulasooriya, Tahira Naseem, Sumit Neelam, Lucian Popa, Revanth Gangi Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G P Shrivatsa Bhargav, and Mo Yu. 2021. Leveraging Abstract Meaning Representation for Knowledge Base Question Answering. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 3884-3894. <a href="https://doi.org/10.18653/v1/2021.findings-acl.339">https://doi.org/10.18653/v1/2021.findings-acl.339</a></p>
<p>[50] Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011. Overview of Genia Event Task in BioNLP Shared Task 2011. In Proceedings of BioNLP Shared Task 2011 Workshop, Jun'ichi Tsujii, Jin-Dong Kim, and Sampo Pyysalo (Eds.). Association for Computational Linguistics, Portland, Oregon, USA, 7-15. <a href="https://aclanthology.org/W11-1802">https://aclanthology.org/W11-1802</a></p>
<p>[51] Knight, Kevin, Badarau, Bianca, Baranescu, Laura, Bonial, Claire, Griffitt, Kira, Hermjakob, Ulf, Marcu, Daniel, O’Gorman, Tim, Palmer, Martha, Schneider, Nathan, and Bardocz, Madalina. 2020. Abstract Meaning Representation (AMR) Annotation Release 3.0. <a href="https://doi.org/10.35111/44CY-BP51">https://doi.org/10.35111/44CY-BP51</a></p>
<p>[52] Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-Sequence Models for Parsing and Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 146-157. <a href="https://doi.org/10.18653/v1/P17-1014">https://doi.org/10.18653/v1/P17-1014</a></p>
<p>[53] Irene Langkilde and Kevin Knight. 1998. Generation that Exploits Corpus-Based Statistical Knowledge. In COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics. <a href="https://aclanthology.org/C98-1112">https://aclanthology.org/C98-1112</a></p>
<p>[54] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. CoRR abs/1910.13461 (2019). arXiv:1910.13461 <a href="http://arxiv.org/abs/1910.13461">http://arxiv.org/abs/1910.13461</a></p>
<p>[55] Bin Li, Yuan Wen, Weiguang Qu, Lijun Bu, and Nianwen Xue. 2016. Annotating the Little Prince with Chinese AMRs. In Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016 (LAW-X 2016), Annemarie Friedrich and Katrin Tomanek (Eds.). Association for Computational Linguistics, Berlin, Germany, 7-15. <a href="https://doi.org/10.18653/v1/W16-1702">https://doi.org/10.18653/v1/W16-1702</a></p>
<p>[56] Changmao Li and Jeffrey Flanigan. 2022. Improving Neural Machine Translation with the Abstract Meaning Representation by Combining Graph and Sequence Transformers. In Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022), Lingfei Wu, Bang Liu, Rada Mihalcea, Jian Pei, Yue Zhang, and Yunyao Li (Eds.). Association for Computational Linguistics, Seattle, Washington, 12-21. <a href="https://doi.org/10.18653/v1/2022.dlg4nlp-1.2">https://doi.org/10.18653/v1/2022.dlg4nlp-1.2</a></p>
<p>[57] Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Grishman. 2015. Improving Event Detection with Abstract Meaning Representation. In Proceedings of the First Workshop on Computing News Storylines, Tommaso Caselli, Marieke van Erp, Anne-Lyse Minard, Mark Finlayson, Ben Miller, Jordi Atserias, Alexandra Balahur, and Piek Vossen (Eds.). Association for Computational Linguistics, Beijing, China, 11-15. <a href="https://doi.org/10.18653/v1/W15-4502">https://doi.org/10.18653/v1/W15-4502</a></p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 29-->
<p>[58] Kexin Liao, Logan Lebanoff, and Fei Liu. 2018. Abstract Meaning Representation for Multi-Document Summarization. In Proceedings of the 27th International Conference on Computational Linguistics, Emily M. Bender, Leon Derczynski, and Pierre Isabelle (Eds.). Association for Computational Linguistics, Santa Fe, New Mexico, USA, 1178-1190. <a href="https://aclanthology.org/C18-1101">https://aclanthology.org/C18-1101</a></p>
<p>[59] Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward Abstractive Summarization Using Semantic Representations. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Rada Mihalcea, Joyce Chai, and Anoop Sarkar (Eds.). Association for Computational Linguistics, Denver, Colorado, 1077-1086. <a href="https://doi.org/10.3115/v1/N15-1114">https://doi.org/10.3115/v1/N15-1114</a></p>
<p>[60] Chunchuan Lyu and Ivan Titov. 2018. AMR Parsing as Graph Prediction with Latent Alignment. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, Melbourne, Australia, 397-407. <a href="https://doi.org/10.18653/v1/P18-1037">https://doi.org/10.18653/v1/P18-1037</a></p>
<p>[61] Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang, Shuang Li, Philip S. Yu, and Lijie Wen. 2023. AMR-based Network for Aspect-based Sentiment Analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 322-337. <a href="https://doi.org/10.18653/v1/2023.acl-long.19">https://doi.org/10.18653/v1/2023.acl-long.19</a></p>
<p>[62] Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A Language-Model-First Approach for AMR-to-Text Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 1846-1852. <a href="https://doi.org/10.18653/v1/2020.acl-main.167">https://doi.org/10.18653/v1/2020.acl-main.167</a></p>
<p>[63] William C. Mann and Eduard H. Hovy. 1989. The Penman Language Generation Project. In Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, Pennsylvania, February 21-23, 1989. <a href="https://aclanthology.org/H89-1021">https://aclanthology.org/H89-1021</a></p>
<p>[64] Behrooz Mansouri, Ricardo Campos, and Adam Jatowt. 2023. Towards Timeline Generation with Abstract Meaning Representation. In Companion Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW '23 Companion). Association for Computing Machinery, New York, NY, USA, 1204-1207. <a href="https://doi.org/10.1145/3543873.3587670">https://doi.org/10.1145/3543873.3587670</a></p>
<p>[65] Behrooz Mansouri, Douglas W. Oard, and Richard Zanibbi. 2022. Contextualized Formula Search Using Math Abstract Meaning Representation. In Proceedings of the 31st ACM International Conference on Information &#x26; Knowledge Management (Atlanta, GA, USA) (CIKM '22). Association for Computing Machinery, New York, NY, USA, 4329-4333. <a href="https://doi.org/10.1145/3511808.3557567">https://doi.org/10.1145/3511808.3557567</a></p>
<p>[66] Noelia Migueles-Abraira, Rodrigo Agerri, and Arantza Diaz de Ilarraza. 2018. Annotating Abstract Meaning Representations for Spanish. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga (Eds.). European Language Resources Association (ELRA), Miyazaki, Japan. https:// aclanthology.org/L18-1486</p>
<p>[67] Ritwik Mishra and Tirthankar Gayen. 2018. Automatic Lossless-Summarization of News Articles with Abstract Meaning Representation. Procedia Computer Science 135 (2018), 178-185. <a href="https://doi.org/10.1016/j.procs.2018.08.164">https://doi.org/10.1016/j.procs.2018.08.164</a> The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life.</p>
<p>[68] Sebastien Montella, Alexis Nasr, Johannes Heinecke, Frederic Bechet, and Lina M. Rojas Barahona. 2023. Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 727-736. <a href="https://doi.org/10.18653/v1/2023.eacl-main.51">https://doi.org/10.18653/v1/2023.eacl-main.51</a></p>
<p>[69] Almuth Müller and Achim Kuwertz. 2022. Evaluation of a Semantic Search Approach based on AMR for Information Retrieval in Image Exploitation. In 2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF). 1-6. <a href="https://doi.org/10.1109/SDF55338.2022.9931702">https://doi.org/10.1109/SDF55338.2022.9931702</a></p>
<p>[70] Tahira Naseem, Austin Blodgett, Sadhana Kumaravel, Tim O'Gorman, Young-Suk Lee, Jeffrey Flanigan, Ramón Astudillo, Radu Florian, Salim Roukos, and Nathan Schneider. 2022. DocAMR: Multi-Sentence AMR Representation and Evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 3496-3505. <a href="https://doi.org/10.18653/v1/2022.naacl-main.256">https://doi.org/10.18653/v1/2022.naacl-main.256</a></p>
<p>[71] Antonio M. S. Almeida Neto, Helena M. Caseli, and Tiago A. Almeida. 2020. Dense Captioning Using Abstract Meaning Representation. In Intelligent Systems, Ricardo Cerri and Ronaldo C. Prati (Eds.). Springer International Publishing, Cham, 450-465.</p>
<p>[72] Long H. B. Nguyen, Viet H. Pham, and Dien Dinh. 2021. Improving Neural Machine Translation with AMR Semantic Graphs. Mathematical Problems in Engineering 2021, 1 (2021), 9939389. <a href="https://doi.org/10.1155/2021/9939389">https://doi.org/10.1155/2021/9939389</a></p>
<p>[73] Juri Opitz. 2023. SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. In Findings of the Association for Computational Linguistics: EACL 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 1595-1607. <a href="https://doi.org/10.18653/v1/2023.findings-eacl.118">https://doi.org/10.18653/v1/2023.findings-eacl.118</a></p>
<p>[74] Juri Opitz, Angel Daza, and Anette Frank. 2021. Weisfeiler-Leman in the Bamboo: Novel AMR Graph Metrics and a Benchmark for AMR Graph Similarity. Transactions of the Association for Computational Linguistics 9 (2021), 1425-1441. <a href="https://doi.org/10.1162/tacl_a_00435">https://doi.org/10.1162/tacl_a_00435</a></p>
<p>[75] Juri Opitz, Letitia Parcalabescu, and Anette Frank. 2020. AMR Similarity Metrics from Principles. Transactions of the Association for Computational Linguistics 8 (2020), 522-538. <a href="https://doi.org/10.1162/tacl_a_00329">https://doi.org/10.1162/tacl_a_00329</a></p><!-- Meanless: 30 Behrooz Mansouri-->
<p>[76] Juri Opitz, Shira Wein, Julius Steen, Anette Frank, and Nathan Schneider. 2023. AMR4NLI: Interpretable and robust NLI measures from semantic graphs. In Proceedings of the 15th International Conference on Computational Semantics, Maxime Amblard and Ellen Breitholtz (Eds.). Association for Computational Linguistics, Nancy, France, 275-283. <a href="https://aclanthology.org/2023.iwcs-1.29">https://aclanthology.org/2023.iwcs-1.29</a></p>
<p>[77] Christoph Otto, Jonas Groschwitz, Alexander Koller, Xiulin Yang, and Lucia Donatelli. 2024. A Corpus of German Abstract Meaning Representation (DeAMR). In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 286-292. <a href="https://aclanthology.org/2024.lrec-main.26">https://aclanthology.org/2024.lrec-main.26</a></p>
<p>[78] Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Comput. Linguist. 31, 1 (March 2005), 71-106. <a href="https://doi.org/10.1162/0891201053630264">https://doi.org/10.1162/0891201053630264</a></p>
<p>[79] Palmer, Martha, Marcu, Daniel, Griffitt, Kira, Knight, Kevin, Baranescu, Laura, Bonial, Claire, Georgescu, Madalina, Hermjakob, Ulf, and Schneider, Nathan. 2014. Abstract Meaning Representation (AMR) Annotation Release 1.0. <a href="https://doi.org/10.35111/0YNC-7404">https://doi.org/10.35111/0YNC-7404</a></p>
<p>[80] Palmer, Martha, Marcu, Daniel, Griffitt, Kira, Knight, Kevin, Baranescu, Laura, Bonial, Claire, Georgescu, Madalina, Hermjakob, Ulf, and Schneider, Nathan. 2014. Abstract Meaning Representation (AMR) Annotation Release 1.0. <a href="https://doi.org/10.35111/0YNC-7404">https://doi.org/10.35111/0YNC-7404</a></p>
<p>[81] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311-318. <a href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a></p>
<p>[82] Hyeonchu Park, Byungjun Kim, and Bugeun Kim. 2025. DART: An AIGT Detector using AMR of Rephrased Text. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 710-721. <a href="https://aclanthology.org/2025.naacl-short.59/">https://aclanthology.org/2025.naacl-short.59/</a></p>
<p>[83] Siyana Pavlova, Maxime Amblard, and Bruno Guillaume. 2023. Structural and Global Features for Comparing Semantic Representation Formalisms. In Proceedings of the Fourth International Workshop on Designing Meaning Representations, Julia Bonn and Nianwen Xue (Eds.). Association for Computational Linguistics, Nancy, France, 1-12. <a href="https://aclanthology.org/2023.dmr-1.1">https://aclanthology.org/2023.dmr-1.1</a></p>
<p>[84] Xiaochang Peng, Chuan Wang, Daniel Gildea, and Nianwen Xue. 2017. Addressing the Data Sparsity Issue in Neural AMR Parsing. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Mirella Lapata, Phil Blunsom, and Alexander Koller (Eds.). Association for Computational Linguistics, Valencia, Spain, 366-375. <a href="https://aclanthology.org/E17-1035">https://aclanthology.org/E17-1035</a></p>
<p>[85] Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer (Eds.). Association for Computational Linguistics, Copenhagen, Denmark, 612-618. <a href="https://doi.org/10.18653/v1/W17-4770">https://doi.org/10.18653/v1/W17-4770</a></p>
<p>[86] Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of the 9th International Natural Language Generation conference, Amy Isard, Verena Rieser, and Dimitra Gkatzia (Eds.). Association for Computational Linguistics, Edinburgh, UK, 21-25. <a href="https://doi.org/10.18653/v1/W16-6603">https://doi.org/10.18653/v1/W16-6603</a></p>
<p>[87] Haoyi Qiu, Kung-Hsiang Huang, Jingnong Qu, and Nanyun Peng. 2024. AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 594-608. <a href="https://doi.org/10.18653/v1/2024.naacl-long.33">https://doi.org/10.18653/v1/2024.naacl-long.33</a></p>
<p>[88] Sudha Rao, Daniel Marcu, Kevin Knight, and Hal Daumé III. 2017. Biomedical Event Extraction using Abstract Meaning Representation. In BioNLP 2017, Kevin Bretonnel Cohen, Dina Demner-Fushman, Sophia Ananiadou, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Vancouver, Canada,, 126-135. <a href="https://doi.org/10.18653/v1/W17-2315">https://doi.org/10.18653/v1/W17-2315</a></p>
<p>[89] Michael Regan, Shira Wein, George Baker, and Emilio Monti. 2024. MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection. arXiv:2405.19285 [cs.CL] <a href="https://arxiv.org/abs/2405.19285">https://arxiv.org/abs/2405.19285</a></p>
<p>[90] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 3982-3992. <a href="https://doi.org/10.18653/v1/D19-1410">https://doi.org/10.18653/v1/D19-1410</a></p>
<p>[91] Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 4269-4282. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.351">https://doi.org/10.18653/v1/2021.emnlp-main.351</a></p>
<p>[92] Zacchary Sadeddine, Juri Opitz, and Fabian Suchanek. 2024. A Survey of Meaning Representations - From Theory to Practical Utility. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 2877-2892. <a href="https://doi.org/10.18653/v1/2024.naacl-long.159">https://doi.org/10.18653/v1/2024.naacl-long.159</a></p>
<p>[93] Ziyi Shou, Yuxin Jiang, and Fangzhen Lin. 2022. AMR-DA: Data Augmentation by Abstract Meaning Representation. In Findings of the Association for Computational Linguistics: ACL 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 3082-3098. <a href="https://doi.org/10.18653/v1/2022.findings-acl.244">https://doi.org/10.18653/v1/2022.findings-acl.244</a></p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 31-->
<p>[94] Ziyi Shou and Fangzhen Lin. 2023. Evaluate AMR Graph Similarity via Self-supervised Learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 16112-16123. <a href="https://doi.org/10.18653/v1/2023.acl-long.892">https://doi.org/10.18653/v1/2023.acl-long.892</a></p>
<p>[95] Linfeng Song and Daniel Gildea. 2019. SemBleu: A Robust Metric for AMR Parsing Evaluation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Anna Korhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 4547-4552. <a href="https://doi.org/10.18653/v1/P19-1446">https://doi.org/10.18653/v1/P19-1446</a></p>
<p>[96] Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic Neural Machine Translation Using AMR. Transactions of the Association for Computational Linguistics 7 (2019), 19-31. <a href="https://doi.org/10.1162/tacl_a_00252">https://doi.org/10.1162/tacl_a_00252</a></p>
<p>[97] Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text Generation with Synchronous Node Replacement Grammar. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 7-13. <a href="https://doi.org/10.18653/v1/P17-2002">https://doi.org/10.18653/v1/P17-2002</a></p>
<p>[98] Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A Graph-to-Sequence Model for AMR-to-Text Generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, Melbourne, Australia, 1616-1626. <a href="https://doi.org/10.18653/v1/P18-1150">https://doi.org/10.18653/v1/P18-1150</a></p>
<p>[99] William Soto Martinez, Yannick Parmentier, and Claire Gardent. 2024. Generating from AMRs into High and Low-Resource Languages using Phylogenetic Knowledge and Hierarchical QLoRA Training (HQL). In Proceedings of the 17th International Natural Language Generation Conference, Saad Mahamood, Nguyen Le Minh, and Daphne Ippolito (Eds.). Association for Computational Linguistics, Tokyo, Japan, 70-81. <a href="https://aclanthology.org/2024.inlg-main.7">https://aclanthology.org/2024.inlg-main.7</a></p>
<p>[100] Afonso Sousa and Henrique Cardoso. [n. d.]. SAPG: Semantically-Aware Paraphrase Generation with AMR Graphs. In Proceedings of the 17th International Conference on Agents and Artificial Intelligence - Volume 2: ICAART. 861-871. <a href="https://doi.org/10.5220/0013379700003890">https://doi.org/10.5220/0013379700003890</a></p>
<p>[101] Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning. arXiv:2008.00401 [cs.CL] <a href="https://arxiv.org/abs/2008.00401">https://arxiv.org/abs/2008.00401</a></p>
<p>[102] Nasim Tohidi, Chitra Dadkhah, Reza Nouralizadeh Ganji, Ehsan Ghaffari Sadr, and Hoda Elmi. 2024. PAMR: Persian Abstract Meaning Representation Corpus. ACM Trans. Asian Low-Resour. Lang. Inf. Process. 23, 3, Article 35 (mar 2024), 20 pages. <a href="https://doi.org/10.1145/3638288">https://doi.org/10.1145/3638288</a></p>
<p>[103] Sarah Uhrig, Yoalli Garcia, Juri Opitz, and Anette Frank. 2021. Translate, then Parse! A Strong Baseline for Cross-Lingual AMR Parsing. In Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), Stephan Oepen, Kenji Sagae, Reut Tsarfaty, Gosse Bouma, Djamé Seddah, and Daniel Zeman (Eds.). Association for Computational Linguistics, Online, 58-64. <a href="https://doi.org/10.18653/v1/2021.iwpt-1.6">https://doi.org/10.18653/v1/2021.iwpt-1.6</a></p>
<p>[104] Jens E. L. Van Gysel, Meagan Vigus, Jayeol Chun, Kenneth Lai, Sarah Moeller, Jiarui Yao, Tim O'Gorman, Andrew Cowell, William Croft, Chu-Ren Huang, Jan Hajič, James H. Martin, Stephan Oepen, Martha Palmer, James Pustejovsky, Rosa Vallejos, and Nianwen Xue. 2021. Designing a Uniform Meaning Representation for Natural Language Processing. KI - Künstliche Intelligenz 35, 3 (01 Nov 2021), 343-360. <a href="https://doi.org/10.1007/s13218-">https://doi.org/10.1007/s13218-</a> 021-00722-w</p>
<p>[105] Lucy Vanderwende, Arul Menezes, and Chris Quirk. 2015. An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Matt Gerber, Catherine Havasi, and Finley Lacatusu (Eds.). Association for Computational Linguistics, Denver, Colorado, 26-30. <a href="https://doi.org/10.3115/v1/N15-3006">https://doi.org/10.3115/v1/N15-3006</a></p>
<p>[106] Pavlo Vasylenko, Pere Lluis Huguet Cabot, Abelardo Carlos Martinez Lorenzo, and Roberto Navigli. 2023. Incorporating Graph Information in Transformer-based AMR Parsing. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 1995-2011. <a href="https://doi.org/10.18653/v1/2023.findings-acl.125">https://doi.org/10.18653/v1/2023.findings-acl.125</a></p>
<p>[107] Supriti Vijay and Daniel Hershcovich. 2024. Can Abstract Meaning Representation Facilitate Fair Legal Judgement Predictions?. In Proceedings of the Fifth Workshop on Insights from Negative Results in NLP, Shabnam Tafreshi, Arjun Akula, João Sedoc, Aleksandr Drozd, Anna Rogers, and Anna Rumshisky (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 101-109. <a href="https://doi.org/10.18653/v1/2024.insights-1.13">https://doi.org/10.18653/v1/2024.insights-1.13</a></p>
<p>[108] Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji, and Nianwen Xue. 2016. CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), Steven Bethard, Marine Carpuat, Daniel Cer, David Jurgens, Preslav Nakov, and Torsten Zesch (Eds.). Association for Computational Linguistics, San Diego, California, 1173-1178. <a href="https://doi.org/10.18653/v1/S16-1181">https://doi.org/10.18653/v1/S16-1181</a></p>
<p>[109] Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A Transition-based Algorithm for AMR Parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Rada Mihalcea, Joyce Chai, and Anoop Sarkar (Eds.). Association for Computational Linguistics, Denver, Colorado, 366-375. <a href="https://doi.org/10.3115/v1/N15-1040">https://doi.org/10.3115/v1/N15-1040</a></p>
<p>[110] Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. AMR-To-Text Generation with Graph Transformer. Transactions of the Association for Computational Linguistics 8 (2020), 19-33. <a href="https://doi.org/10.1162/tacl_a_00297">https://doi.org/10.1162/tacl_a_00297</a></p>
<p>[111] Tianming Wang, Xiaojun Wan, and Shaowei Yao. 2020. Better AMR-To-Text Generation with Graph Structure Reconstruction. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, Christian Bessiere (Ed.). International Joint Conferences on Artificial Intelligence Organization, 3919-3925. <a href="https://doi.org/10.24963/ijcai.2020/542">https://doi.org/10.24963/ijcai.2020/542</a> Main track.</p>
<p>[112] Yanshan Wang, Sijia Liu, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Fei Liu, and Hongfang Liu. 2017. Dependency and AMR Embeddings for Drug-Drug Interaction Extraction from Biomedical Literature. In Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics (Boston,Massachusetts,USA) (ACM-BCB '17). Association for Computing Machinery,New York,NY, USA, 36-43. <a href="https://doi.org/10.1145/3107411.3107426">https://doi.org/10.1145/3107411.3107426</a></p><!-- Meanless: 32 Behrooz Mansouri-->
<p>[113] Shira Wein and Nathan Schneider. 2021. Classifying Divergences in Cross-lingual AMR Pairs. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, Claire Bonial and Nianwen Xue (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 56-65. <a href="https://doi.org/10.18653/v1/2021.law-1.6">https://doi.org/10.18653/v1/2021.law-1.6</a></p>
<p>[114] Shira Wein and Nathan Schneider. 2024. Assessing the Cross-linguistic Utility of Abstract Meaning Representation. Computational Linguistics (2024), 1-55.</p>
<p>[115] Shira Wein and Nathan Schneider. 2024. Lost in Translationese? Reducing Translation Effect Using Abstract Meaning Representation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julian's, Malta, 753-765. <a href="https://aclanthology.org/2024.eacl-long.45">https://aclanthology.org/2024.eacl-long.45</a></p>
<p>[116] Aaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2016. Universal Decompositional Semantics on Universal Dependencies. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics, Austin, Texas, 1713-1723. <a href="https://doi.org/10.18653/v1/D16-1177">https://doi.org/10.18653/v1/D16-1177</a></p>
<p>[117] Gregor Williamson, Patrick Elliott, and Yuxin Ji. 2021. Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, Claire Bonial and Nianwen Xue (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 160-169. <a href="https://doi.org/10.18653/v1/2021.law-1.17">https://doi.org/10.18653/v1/2021.law-1.17</a></p>
<p>[118] Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and Guodong Zhou. 2021. XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 896-907. <a href="https://doi.org/10.18653/v1/2021.acl-long.73">https://doi.org/10.18653/v1/2021.acl-long.73</a></p>
<p>[119] Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang, and Zhifang Sui. 2022. A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 5025-5036. <a href="https://doi.org/10.18653/v1/2022.naacl-main.370">https://doi.org/10.18653/v1/2022.naacl-main.370</a></p>
<p>[120] Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer, and Xiuhong Zhang. 2014. Not an Interlingua, But Close: Comparison of English AMRs to Chinese and Czech. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), Vol. 14. 1765-1772. <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/384_Paper.pdf">http://www.lrec-conf.org/proceedings/lrec2014/pdf/384_Paper.pdf</a></p>
<p>[121] Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019. AMR Parsing as Sequence-to-Graph Transduction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Anna Korhonen, David Traum, and Lluis Màrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 80-94. <a href="https://doi.org/10.18653/v1/P19-1009">https://doi.org/10.18653/v1/P19-1009</a></p>
<p>[122] Zixuan Zhang, Nikolaus Parulian, Heng Ji, Ahmed Elsayed, Skatje Myers, and Martha Palmer. 2021. Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 6261-6270. <a href="https://doi.org/10.18653/v1/2021.acl-long.489">https://doi.org/10.18653/v1/2021.acl-long.489</a></p>
<p>[123] Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, and Radu Florian. 2021. AMR Parsing with Action-Pointer Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Kristina Toutanova, Anna Rumshikky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5585-5598. <a href="https://doi.org/10.18653/v1/2021.naacl-main.443">https://doi.org/10.18653/v1/2021.naacl-main.443</a></p>
<p>[124] Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, Young-Suk Lee, Radu Florian, and Salim Roukos. 2021. Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6279-6290. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.507">https://doi.org/10.18653/v1/2021.emnlp-main.507</a></p>
<p>[125] Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling Graph Structure in Transformer for Better AMR-to-Text Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 5459-5468. <a href="https://doi.org/10.18653/v1/D19-1548">https://doi.org/10.18653/v1/D19-1548</a></p><!-- Meanless: Survey of Abstract Meaning Representation: Then, Now, Future 33-->
<h2>A RESOURCES AND TOOLS</h2>
<p>Table 3 provides a list of AMR resources and tools.</p>
<!-- Media -->
<p>Table 3. Selected Papers and Resources for Abstract Meaning Representation.</p>
<table><tbody><tr><td>Model</td><td>Application</td><td>Link</td><td>Year</td></tr><tr><td colspan="4">Parsers and Generators</td></tr><tr><td>JAMR [32]</td><td>Parser, Generator</td><td>https://github.com/jflanigan/jamr</td><td>2014</td></tr><tr><td>CAMR [108]</td><td>Parser, Generator</td><td>https://github.com/c-amr/camr</td><td>2016</td></tr><tr><td>AMR-Eager [23]</td><td>Parser-Multilingual, Generator</td><td>https://github.com/mdtux89/amr-eager</td><td>2017</td></tr><tr><td>NeuralAMR [52]</td><td>Parser, Generator</td><td>https://github.com/sinantie/NeuralAmr</td><td>2017</td></tr><tr><td>AMR-GP [60]</td><td>Parser</td><td>https://github.com/ChunchuanLv/AMR_AS_GRAPH_PREDICTION</td><td>2018</td></tr><tr><td>Graph-to-sequence [98]</td><td>Generator</td><td>https://github.com/freesunshine0316/neural-graph-to-seq-mp</td><td>2018</td></tr><tr><td>Structural Transformer [125]</td><td>Generator</td><td>https://github.com/Amazing-J/structural-transformer</td><td>2019</td></tr><tr><td>STOG [121]</td><td>Parser</td><td>https://github.com/sheng-z/stog</td><td>2019</td></tr><tr><td>GraphTransformer [110]</td><td>Generator</td><td>https://github.com/sodawater/GraphTransformer</td><td>2020</td></tr><tr><td>Graph Reconstruction [111]</td><td>Generator</td><td>https://github.com/sodawater/graph-reconstruction-amr2text</td><td>2020</td></tr><tr><td>GPT-too [62]</td><td>Generator</td><td>https://github.com/IBM/GPT-too-AMR2text</td><td>2020</td></tr><tr><td>XLPT-AMR [118]</td><td>Parser, Generator-Multilingual</td><td>https://github.com/xdqkid/XLPT-AMR</td><td>2021</td></tr><tr><td>XAMR [16]</td><td>Parser-Multilingual</td><td>https://github.com/jcyk/XAMR</td><td>2021</td></tr><tr><td>APT [123]</td><td>Parser</td><td>https://github.com/IBM/transition-amr-parser</td><td>2021</td></tr><tr><td>SPRING [9]</td><td>Parser, Generator</td><td>https://github.com/SapienzaNLP/spring</td><td>2021</td></tr><tr><td>StructAdapt [91]</td><td>Generator</td><td>https://github.com/UKPLab/StructAdapt</td><td>2021</td></tr><tr><td>AMRBART [6]</td><td>Parser, Generator</td><td>https://github.com/goodbai-nlp/AMRBART</td><td>2022</td></tr><tr><td>BiBL[18]</td><td>Parser, Generator</td><td>https://github.com/KHAKhazeus/BiBL</td><td>2022</td></tr><tr><td>XL-AMR [11]</td><td>Parser-Multilingual</td><td>https://github.com/SapienzaNLP/xl-amr</td><td>2022</td></tr><tr><td>EnrichedAMR [46]</td><td>Enrichment</td><td>https://github.com/emorynlp/EnrichedAMR/</td><td>2022</td></tr><tr><td>LeaKDistill [106]</td><td>Parsing</td><td>https://github.com/sapienzanlp/LeakDistill</td><td>2023</td></tr><tr><td>Meta-XMAR [48]</td><td>Parser-Multilingual</td><td>https://github.com/Emvista/Meta-XAMR-2024</td><td>2024</td></tr><tr><td colspan="4">Evaluation Tools</td></tr><tr><td>SMatch [17]</td><td>Evaluation</td><td>https://github.com/snowblink14/SMatch</td><td>2013</td></tr><tr><td>SemBlue [95]</td><td>Evaluation</td><td>https://github.com/freesunshine0316/sembleu</td><td>2019</td></tr><tr><td>SEMA [3]</td><td>Evaluation</td><td>https://github.com/rafaelanchieta/sema</td><td>2019</td></tr><tr><td>\( {\mathrm{S}}^{2} \) match [75]</td><td>Evaluation</td><td>https://github.com/Heidelberg-NLP/amr-metric-suite</td><td>2020</td></tr><tr><td>(WWLK) [74]</td><td>Evaluation</td><td>https://github.com/Heidelberg-NLP/weisfeiler-leman-bamboc</td><td>2021</td></tr><tr><td>\( {\mathrm{{XS}}}^{2} \) match [29]</td><td>Evaluation-Multilingual</td><td>https://github.com/shirawein/Crossling-AMR-Eval</td><td>2022</td></tr><tr><td>SMatch++ [73]</td><td>Evaluation</td><td>https://github.com/flipz357/SMatchpf</td><td>2023</td></tr><tr><td>AMRSim [94]</td><td>Evaluation</td><td>https://github.com/zzshou/AMRSim</td><td>2023</td></tr><tr><td>GrAPES [35]</td><td>Evaluation</td><td>https://github.com/jgroschwitz/GrAPES</td><td>2023</td></tr><tr><td colspan="4">Corpus</td></tr><tr><td>AMR 1.0 [79]</td><td>Corpus</td><td>https://catalog.ldc.upenn.edu/LDC2014T12</td><td>2014</td></tr><tr><td>AMR 2.0 [80]</td><td>Corpus</td><td>https://catalog.ldc.upenn.edu/LDC2017T10</td><td>2017</td></tr><tr><td>AMR 3.0 [51]</td><td>Corpus</td><td>https://catalog.ldc.upenn.edu/LDC2020T02</td><td>2020</td></tr><tr><td>CHAMR [55]</td><td>Corpus-Chinese</td><td>https://www.cs.brandeis.edu/~clp/camr.html</td><td>2016</td></tr><tr><td>AMR 2.0 - 4 translation [20]</td><td>Corpus-Multilingual</td><td>https://hdl.handle.net/11272.1/AB2/5OU0AQ</td><td>2022</td></tr><tr><td>MASSIVE-AMR [89]</td><td>Corpus-Multilingual</td><td>https://github.com/amazon-science/MASSIVE-AMR</td><td>2024</td></tr><tr><td>DeAMR [77]</td><td>Corpus-German</td><td>https://github.com/chriott/DeAMR/</td><td>2024</td></tr><tr><td colspan="4">Applications</td></tr><tr><td>SemanticSummarizer [59]</td><td>Summarization</td><td>https://github.com/summarization/semantic_summ</td><td>2015</td></tr><tr><td>AMR2Text-Summarizer [37]</td><td>Summarization</td><td>https://github.com/sheffieldnlp/AMR2Text-summ</td><td>2018</td></tr><tr><td>TSAR [119]</td><td>Event Detection</td><td>https://github.com/PKUnlp-icler/TSAR</td><td>2022</td></tr><tr><td>Knowledge-AMR [122]</td><td>Information Extraction</td><td>https://github.com/zhangzx-uiuc/Knowledge-AMR</td><td>2021</td></tr><tr><td>Densecap-amr [71]</td><td>Dense Captioning</td><td>https://github.com/LALIC-UFSCar/densecap-amr</td><td>2020</td></tr><tr><td>APARN [61]</td><td>Sentiment Analysis</td><td>https://github.com/THU-BPM/APARN</td><td>2023</td></tr><tr><td>AMR4NLI [76]</td><td>Natural Language Inference</td><td>https://github.com/flipz357/AMR4NLI</td><td>2023</td></tr><tr><td>AMR-DA [93]</td><td>Data Augmentation</td><td>https://github.com/zzshou/amr-data-augmentation</td><td>2022</td></tr><tr><td>AMR-NMT [56]</td><td>Machine Translation</td><td>https://github.com/jlab-nlp/amr-nmt</td><td>2022</td></tr><tr><td>AMR-Translationese [115]</td><td>Machine Translation</td><td>https://github.com/shirawein/amr-translationese</td><td>2024</td></tr><tr><td>ABEX [33]</td><td>Data Augmentation</td><td>https://github.com/Sreyan88/ABEX</td><td>2024</td></tr><tr><td>SAPG [100]</td><td>Paraphrase Generation</td><td>https://github.com/afonso-sousa/sapg</td><td>2025</td></tr></tbody></table>
<!-- Media -->
      </body>
    </html>
  